<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lab3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="Lab3_files/libs/clipboard/clipboard.min.js"></script>
<script src="Lab3_files/libs/quarto-html/quarto.js"></script>
<script src="Lab3_files/libs/quarto-html/popper.min.js"></script>
<script src="Lab3_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Lab3_files/libs/quarto-html/anchor.min.js"></script>
<link href="Lab3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Lab3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Lab3_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Lab3_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Lab3_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="lab-3-gesture-recognition-using-convolutional-neural-networks" class="level1">
<h1>Lab 3: Gesture Recognition using Convolutional Neural Networks</h1>
<p>In this lab you will train a convolutional neural network to make classifications on different hand gestures. By the end of the lab, you should be able to:</p>
<ol type="1">
<li>Load and split data for training, validation and testing</li>
<li>Train a Convolutional Neural Network</li>
<li>Apply transfer learning to improve your model</li>
</ol>
<p>Note that for this lab we will not be providing you with any starter code. You should be able to take the code used in previous labs, tutorials and lectures and modify it accordingly to complete the tasks outlined below.</p>
<section id="what-to-submit" class="level3">
<h3 class="anchored" data-anchor-id="what-to-submit">What to submit</h3>
<p>Submit a PDF file containing all your code, outputs, and write-up from parts 1-5. You can produce a PDF of your Google Colab file by going to <strong>File &gt; Print</strong> and then save as PDF. The Colab instructions has more information. Make sure to review the PDF submission to ensure that your answers are easy to read. Make sure that your text is not cut off at the margins.</p>
<p><strong>Do not submit any other files produced by your code.</strong></p>
<p>Include a link to your colab file in your submission.</p>
<p>Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission.</p>
</section>
<section id="colab-link" class="level2">
<h2 class="anchored" data-anchor-id="colab-link">Colab Link</h2>
<p>Include a link to your colab file here</p>
<p>Colab Link:</p>
<p><a href="https://colab.research.google.com/github/GreatArcStudios/APS360/blob/master/Lab%203/Lab3.ipynb">https://colab.research.google.com/github/GreatArcStudios/APS360/blob/master/Lab%203/Lab3.ipynb</a></p>
</section>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>American Sign Language (ASL) is a complete, complex language that employs signs made by moving the hands combined with facial expressions and postures of the body. It is the primary language of many North Americans who are deaf and is one of several communication options used by people who are deaf or hard-of-hearing. The hand gestures representing English alphabet are shown below. This lab focuses on classifying a subset of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand showing one of the letters A-I, we want to detect which letter is being represented.</p>
<p><img src="https://www.disabled-world.com/pics/1/asl-alphabet.jpg" class="img-fluid"></p>
</section>
<section id="part-b.-building-a-cnn-50-pt" class="level2">
<h2 class="anchored" data-anchor-id="part-b.-building-a-cnn-50-pt">Part B. Building a CNN [50 pt]</h2>
<p>For this lab, we are not going to give you any starter code. You will be writing a convolutional neural network from scratch. You are welcome to use any code from previous labs, lectures and tutorials. You should also write your own code.</p>
<p>You may use the PyTorch documentation freely. You might also find online tutorials helpful. However, all code that you submit must be your own.</p>
<p>Make sure that your code is vectorized, and does not contain obvious inefficiencies (for example, unecessary for loops, or unnecessary calls to unsqueeze()). Ensure enough comments are included in the code so that your TA can understand what you are doing. It is your responsibility to show that you understand what you write.</p>
<p><strong>This is much more challenging and time-consuming than the previous labs.</strong> Make sure that you give yourself plenty of time by starting early.</p>
<section id="data-loading-and-splitting-5-pt" class="level3">
<h3 class="anchored" data-anchor-id="data-loading-and-splitting-5-pt">1. Data Loading and Splitting [5 pt]</h3>
<p>Download the anonymized data provided on Quercus. To allow you to get a heads start on this project we will provide you with sample data from previous years. Split the data into training, validation, and test sets.</p>
<p>Note: Data splitting is not as trivial in this lab. We want our test set to closely resemble the setting in which our model will be used. In particular, our test set should contain hands that are never seen in training!</p>
<p>Explain how you split the data, either by describing what you did, or by showing the code that you used. Justify your choice of splitting strategy. How many training, validation, and test images do you have?</p>
<p>For loading the data, you can use plt.imread as in Lab 1, or any other method that you choose. You may find torchvision.datasets.ImageFolder helpful. (see https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=image%20folder#torchvision.datasets.ImageFolder )</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># some dependencies for fancy progress bar and model summary</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> pip install tqdm </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> pip install torchinfo</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># imports</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, random_split</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> trange</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataset and dataloaders</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_datasets(small_set_adjustment<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> [torchvision.transforms.ToTensor(</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    ), torchvision.transforms.Normalize((<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), (<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> torchvision.transforms.Compose(transforms)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    images_dataset <span class="op">=</span> torchvision.datasets.ImageFolder(</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"./Dataset/Lab3_Gestures_Summer/"</span>, transform<span class="op">=</span>transforms)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    leftover_prob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> <span class="fl">0.7</span> <span class="op">*</span> small_set_adjustment <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        small_set_adjustment <span class="op">-</span> <span class="fl">0.2</span> <span class="op">*</span> small_set_adjustment</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    split_sets <span class="op">=</span> random_split(images_dataset, [</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                              <span class="fl">0.7</span> <span class="op">*</span> small_set_adjustment, <span class="fl">0.1</span> <span class="op">*</span> small_set_adjustment, <span class="fl">0.2</span> <span class="op">*</span> small_set_adjustment, leftover_prob])</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> split_sets</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_dataloaders(split_sets, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, use_cuda<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    train_set, val_set, test_set <span class="op">=</span> split_sets</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> DataLoader(</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        train_set, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span>shuffle, pin_memory<span class="op">=</span>use_cuda)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> DataLoader(val_set, batch_size<span class="op">=</span><span class="bu">len</span>(</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        val_set), shuffle<span class="op">=</span>shuffle, pin_memory<span class="op">=</span>use_cuda)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    test_loader <span class="op">=</span> DataLoader(test_set, batch_size<span class="op">=</span><span class="bu">len</span>(</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        test_set), shuffle<span class="op">=</span>shuffle, pin_memory<span class="op">=</span>use_cuda)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader, test_loader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4th set is a leftover set if we are trying to create a small subset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    train_set, val_set, test_set, _ <span class="op">=</span> create_datasets()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    split_sets <span class="op">=</span> (train_set, val_set, test_set)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">len</span>(train_set), <span class="bu">len</span>(val_set), <span class="bu">len</span>(test_set))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I split the data using a 70-10-20 training, validation, test split, meaning I could use 10% of the data to tune my hyperparameters and estimate the training error (usually training error is somewhat below the validation error). Further, I chose to use a 20% test set since this would give us a better approximation of generalization performance than using something like a 10% test set. I didn’t want to go with a smaller test set to increase the training set size because I wanted to see more reliability if I ended up with a good model. Ultimately, I have 2220 samples overall, meaning 1554 samples in the training set, 222 samples in the validation set, and finally 444 samples in the test set. I also chose to use the pytorch <code>ImageFolder</code> dataset API for creating the datasets since it allowed me to use the <code>random_split</code> function, which accepts a vector of probabilities (proportions) when sampling the train/validation/test sets. This allows us to conveniently control for the average number of samples randomly included in each dataset; consider that the expectation for some class <span class="math inline">\(i\)</span> with probability <span class="math inline">\(p_i\)</span> of a multinomial distribution is <span class="math inline">\(n \cdot p_i\)</span>, i.e., <span class="math inline">\(\mathbb{E}\left[ x_i \right] = n\cdot p_i\)</span>. Then I used the dataloader API to create dataloaders, which is advantageous since it has support for pinned memory, which can help with copies between the CPU and GPU and easy control over batch size and data shuffling.</p>
</section>
<section id="model-building-and-sanity-checking-15-pt" class="level3">
<h3 class="anchored" data-anchor-id="model-building-and-sanity-checking-15-pt">2. Model Building and Sanity Checking [15 pt]</h3>
</section>
<section id="part-a-convolutional-network---5-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-a-convolutional-network---5-pt">Part (a) Convolutional Network - 5 pt</h3>
<p>Build a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the gesture letter. Your model should be a subclass of nn.Module. Explain your choice of neural network architecture: how many layers did you choose? What types of layers did you use? Were they fully-connected or convolutional? What about other decisions like pooling layers, activation functions, number of channels / hidden units?</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleCNN(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, conv_dilation<span class="op">=</span><span class="dv">2</span>, dropout <span class="op">=</span> <span class="fl">0.05</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_embeddings <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>            <span class="co"># produces shape 9, 200, 200</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">12</span>, <span class="dv">26</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># batch norm to help model training - normalize data</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">12</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># produces shape 9, 99, 99</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(<span class="dv">4</span>, <span class="dv">2</span>),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># produces shape 4, 93, 93</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># expand receptive field</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">12</span>, <span class="dv">8</span>, <span class="dv">3</span>, dilation<span class="op">=</span>conv_dilation),</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">8</span>),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get the most intensive more "long range" features</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">3</span>),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># expand receptive field</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">2</span>, dilation<span class="op">=</span>conv_dilation),</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">4</span>),</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(<span class="dv">2</span>),</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            nn.Mish())</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># programmatically get feature embedding size </span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._feature_embed_size <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.feature_embeddings(torch.rand(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)).detach().size())</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding_size <span class="op">=</span> np.prod(<span class="va">self</span>._feature_embed_size)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use dropout to regularize model </span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># autoencoder like architecture - just remove reconstruction layers</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act1 <span class="op">=</span> nn.Mish()</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act2 <span class="op">=</span> nn.Mish()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># low dimensional latent space</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act3 <span class="op">=</span> nn.Mish()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act4 <span class="op">=</span> nn.Mish()</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc5 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>, <span class="dv">9</span>)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create latent space rep.</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.flatten(<span class="va">self</span>.feature_embeddings(<span class="bu">input</span>))</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.dropout(z)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        z_1 <span class="op">=</span> <span class="va">self</span>.act1(<span class="va">self</span>.fc1(z))</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        z_2 <span class="op">=</span> <span class="va">self</span>.dropout(z_1)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        z_2 <span class="op">=</span> <span class="va">self</span>.act2(<span class="va">self</span>.fc2(z_2))</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        z_3 <span class="op">=</span> <span class="va">self</span>.dropout(z_2)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        z_3 <span class="op">=</span> <span class="va">self</span>.act3(<span class="va">self</span>.fc3(z_3))</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        z_4 <span class="op">=</span> <span class="va">self</span>.dropout(z_3)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        z_4 <span class="op">=</span> <span class="va">self</span>.act4(<span class="va">self</span>.fc4(z_4) <span class="op">+</span> z_1) <span class="co"># skip connection</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        z_5 <span class="op">=</span> <span class="va">self</span>.dropout(z_4)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc5(z_5)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(layer): </span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(layer, nn.Linear):</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        torch.nn.init.xavier_normal_(layer.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I constructed my neural network first with a stack of convolution and pooling layers. This was done to get compressed latent feature representations that I could then feed into various linear layers motivated by the autoencoder architecture, but with the final reconstruction layers removed. I decided on adding quite a few convolution layers because I wanted to feed higher level feature representations into the fully connected layers, which would be further tuned in the latent space by the fully connected layers. Also, I chose to output 9 feature representations (output channel dimension of 9) in the first convolution layer because just wanted some extra granularity in learned kernels (more learned kernels) since each feature representation is only generated by one kernel. I used a dilated convolution since traditional convolution kernels considers only adjacent pixels, and so I used the dilated convolution to increase the receptive field, which allows for me to capture the potentially useful dependencies between further apart elements in the feature representations. I used average pooling after the first convolution layer since I wanted to average out “sharp” signals rather than have the network focus on the most intense values of the low level features. My final pooling layer also used average pooling for the same reason but for higher level features; I wanted the fully connected layers to be fed what was “generally” going on in some image. I used dropout to help prevent overfitting of the rather dense fully connected layers (the first one has more than half of the total parameters), and allows for essentially model averaging (<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Srivastava et al.</a>). Further, my linear layers were constructed in an autoencoder like fashion, i.e., we project into a smaller latent space, forcing the network to further learn what was actually important, and then projected it back up for the final output layer. I also added a skip connection for the <code>z_4</code> layer (<code>self.fc4(z_4) + z_1</code>) in order to help mitigate the vanishing gradient problem, and “remind” the higher dimensional projection (<code>z_4</code>) from the smaller latent space (<code>z_3</code>) of a latent representation closer to that of the convolution feature representations (<code>z_1</code>). Finally, I used the Mish activation since it can create much smoother loss landscapes and self-regularizes (<a href="https://arxiv.org/pdf/1908.08681.pdf">Misra</a>).</p>
</section>
<section id="part-b-training-code---5-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-b-training-code---5-pt">Part (b) Training Code - 5 pt</h3>
<p>Write code that trains your neural network given some training data. Your training code should make it easy to tweak the usual hyperparameters, like batch size, learning rate, and the model object itself. Make sure that you are checkpointing your models from time to time (the frequency is up to you). Explain your choice of loss function and optimizer.</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_loss(preds, targets):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> criterion(preds, targets)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(train_loader, val_loader, convolution_dilation<span class="op">=</span><span class="dv">2</span>, epochs<span class="op">=</span><span class="dv">1000</span>, learning_rate<span class="op">=</span><span class="fl">0.0095</span>, momentum<span class="op">=</span><span class="fl">0.95</span>, wd<span class="op">=</span><span class="fl">1e-4</span>, batch_size<span class="op">=</span><span class="va">None</span>, train_data<span class="op">=</span><span class="va">None</span>, use_cuda<span class="op">=</span><span class="va">True</span>, use_tqdm<span class="op">=</span><span class="va">False</span>, epoch_save_start<span class="op">=</span><span class="dv">100</span>, use_lr_sched<span class="op">=</span><span class="va">True</span>, epoch_slow_lr_start<span class="op">=</span><span class="dv">120</span>, slow_lr<span class="op">=</span><span class="fl">0.005</span>, low_momentum<span class="op">=</span><span class="fl">0.89</span>, print_every<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine if CUDA is available and set Tensor core flags</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        dev <span class="op">=</span> <span class="st">"cuda:0"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"CUDA unavailable, training on CPU"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        dev <span class="op">=</span> <span class="st">"CPU"</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(dev)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    network <span class="op">=</span> SimpleCNN(conv_dilation<span class="op">=</span>convolution_dilation)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    network <span class="op">=</span> network.to(device)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    network.<span class="bu">apply</span>(init_weights)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(network.parameters(</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    ), lr<span class="op">=</span>learning_rate, momentum<span class="op">=</span>momentum, weight_decay<span class="op">=</span>wd, nesterov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use for Nvidia AMP training cycle</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> torch.cuda.amp.GradScaler()</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train_data <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        summary(network, input_data<span class="op">=</span>train_data, verbose<span class="op">=</span><span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    loss_dict <span class="op">=</span> {<span class="st">"config"</span>: <span class="ss">f"Convolution Dilation: </span><span class="sc">{</span>convolution_dilation<span class="sc">}</span><span class="ss">, Epochs: </span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">, Lr: </span><span class="sc">{</span>learning_rate<span class="sc">}</span><span class="ss">, Momentum:</span><span class="sc">{</span>momentum<span class="sc">}</span><span class="ss">, Weight Decay: </span><span class="sc">{</span>wd<span class="sc">}</span><span class="ss">, Batch Size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"train_loss"</span>: [], <span class="st">"val_loss"</span>: [],</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"train_acc"</span>: [], <span class="st">"val_acc"</span>: []}</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># counter for determining checkpoints</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    best_val_acc <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run_epoch():</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        network.train()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        epoch_loss, val_loss <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        train_correct, train_total <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        val_correct, val_total <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># reenable train mode to enable dropout</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>            inputs, targets <span class="op">=</span> batch</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> inputs.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> targets.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>            network.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># use Nvidia AMP for tensor cores speed up.</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16):</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>                preds <span class="op">=</span> network(inputs)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> compute_loss(preds, targets)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>            scaler.scale(loss).backward()</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>            scaler.step(optimizer)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>            scaler.update()</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># evaluate the model for each batch</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co">: we evaluate the model on the logits since the logit function is the inverse </span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># of the logistic function (or softmax) and thus is a monotone transformation of </span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># the actual predicted probabilities. </span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>                <span class="co"># eval mode to disable dropout</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>                network.<span class="bu">eval</span>()</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>                epoch_loss <span class="op">+=</span> loss</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>                _, category_preds <span class="op">=</span> torch.<span class="bu">max</span>(preds, <span class="dv">1</span>)</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>                train_correct <span class="op">+=</span> (category_preds <span class="op">==</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>                                  targets).<span class="bu">sum</span>()</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>                train_total <span class="op">+=</span> preds.size()[<span class="dv">0</span>]</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>                <span class="co"># print(loss.item())</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># evaluate the validation set and save the epoch statistics</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>            network.<span class="bu">eval</span>()</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>            <span class="co"># only one batch per val loader</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(val_loader):</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>                inputs, targets <span class="op">=</span> batch</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>                inputs <span class="op">=</span> inputs.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>                targets <span class="op">=</span> targets.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>                preds <span class="op">=</span> network(inputs)</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>                batch_val_loss <span class="op">=</span> compute_loss(preds, targets)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> batch_val_loss</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>                _, category_preds <span class="op">=</span> torch.<span class="bu">max</span>(preds, <span class="dv">1</span>)</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>                val_correct <span class="op">+=</span> (category_preds <span class="op">==</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>                                targets).<span class="bu">sum</span>()</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>                val_total <span class="op">+=</span> preds.size()[<span class="dv">0</span>]</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>                loss_dict[<span class="st">"val_loss"</span>].append(batch_val_loss)</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>                loss_dict[<span class="st">"val_acc"</span>].append(val_correct<span class="op">/</span>val_total)</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>                loss_dict[<span class="st">"train_loss"</span>].append(epoch_loss)</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>                loss_dict[<span class="st">"train_acc"</span>].append(train_correct<span class="op">/</span>train_total)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> epoch_loss, train_correct, train_total, val_loss, val_correct, val_total</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the fancy TQDM progress bar slows down model training likely due to GPU -&gt; CPU copies</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_tqdm:</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> trange(epochs, desc<span class="op">=</span><span class="st">"Train epochs"</span>, unit<span class="op">=</span><span class="st">"epoch"</span>) <span class="im">as</span> train_bar:</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> epoch <span class="kw">in</span> train_bar:</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>                epoch_loss, train_correct, train_total, val_loss, val_correct, val_total <span class="op">=</span> run_epoch()</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>                train_bar.set_postfix(epoch_loss<span class="op">=</span>epoch_loss,</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>                                      train_acc<span class="op">=</span>train_correct<span class="op">/</span>train_total,</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>                                      train_correct<span class="op">=</span>train_correct,</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>                                      train_total<span class="op">=</span>train_total,</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>                                      val_loss<span class="op">=</span>val_loss,</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>                                      val_acc<span class="op">=</span>val_correct<span class="op">/</span>val_total)</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>            <span class="co"># run the epoch</span></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>            epoch_loss, train_correct, train_total, val_loss, val_correct, val_total <span class="op">=</span> run_epoch()</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>            <span class="co"># rudimentary learning rate scheduler</span></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> use_lr_sched <span class="kw">and</span> epoch <span class="op">&gt;=</span> epoch_slow_lr_start:</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> param <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>                    param[<span class="st">"lr"</span>] <span class="op">=</span> slow_lr</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>                    param[<span class="st">"momentum"</span>] <span class="op">=</span> low_momentum</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>            <span class="co"># model check point based on validation accuracy</span></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> torch.<span class="bu">round</span>(val_correct<span class="op">/</span>val_total, decimals<span class="op">=</span><span class="dv">3</span>) <span class="op">&gt;</span> best_val_acc <span class="kw">and</span> epoch <span class="op">&gt;=</span> epoch_save_start:</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>                torch.save(network.state_dict(</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>                ), <span class="ss">f"./models/valacc_</span><span class="sc">{</span>val_correct<span class="op">/</span>val_total<span class="sc">}</span><span class="ss">-convdial_</span><span class="sc">{</span>convolution_dilation<span class="sc">}</span><span class="ss">-lr_</span><span class="sc">{</span>learning_rate<span class="sc">}</span><span class="ss">-momentum_</span><span class="sc">{</span>momentum<span class="sc">}</span><span class="ss">-batch_size_</span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">-epoch_num_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.mdlckpt"</span>)</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>                best_val_acc <span class="op">=</span> torch.<span class="bu">round</span>(val_correct<span class="op">/</span>val_total, decimals<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> print_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> stats: train loss </span><span class="sc">{</span>epoch_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, train acc </span><span class="sc">{</span>(train_correct<span class="op">/</span>train_total)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, val loss </span><span class="sc">{</span>val_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, val acc </span><span class="sc">{</span>(val_correct<span class="op">/</span>val_total)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> network, loss_dict</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_test_performance(network, test_loader):</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a>    test_correct, test_total <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="bu">next</span>(network.parameters()).device</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>        <span class="co"># disable dropout for test time</span></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>        network.<span class="bu">eval</span>()</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(test_loader):</span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>            inputs, targets <span class="op">=</span> batch</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> inputs.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> targets.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> network(inputs)</span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> compute_loss(preds, targets).item()</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>            _, category_preds <span class="op">=</span> torch.<span class="bu">max</span>(preds, <span class="dv">1</span>)</span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(category_preds, targets)</span></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>            test_correct <span class="op">+=</span> <span class="bu">float</span>((category_preds <span class="op">==</span> targets).<span class="bu">sum</span>().item())</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>            test_total <span class="op">+=</span> <span class="bu">len</span>(targets)</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_correct<span class="op">/</span>test_total, test_correct, test_total, test_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I chose cross entropy loss since our task is a mutliclass prediction task, and so we essentially need to output logits to define a multinomial distribution for the hand signs. In other words, our task is to fit a conditional distribution that is a vector of probabilities when conditioned on some image, i.e., for some image <span class="math inline">\(\mathbf{x}\)</span> and some vector of hand sign classes <span class="math inline">\(\mathbf{c}\)</span>, we fit <span class="math inline">\(p(\mathbf{c} \mid \mathbf{x})\)</span>. Cross entropy loss allows us to do this from an output layer (<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">(https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</a>).</p>
<p>Then, I chose SGD as the optimizer because SGD is theoretically guaranteed to converge to the minimum norm solution (assuming a convex objective function, e.g., linear regression), while adaptive optimizers like Adam are not guaranteed to do so (<a href="https://arxiv.org/pdf/1904.09237.pdf">Reddi et al.</a>). Additionally, SGD also implements nesterov momentum, so it has the added benefit of potentially being able to escape certain local minima. In particular, I like how nesterov momentum jumps in the direction of the accumulated gradients then corrects for it since it seems that this network and data result in a tricky loss landscape where there are fairly “deep” local minima (and SGD sometimes needs a gentle extra push).</p>
</section>
<section id="part-c-overfit-to-a-small-dataset---5-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-c-overfit-to-a-small-dataset---5-pt">Part (c) “Overfit” to a Small Dataset - 5 pt</h3>
<p>One way to sanity check our neural network model and training code is to check whether the model is capable of “overfitting” or “memorizing” a small dataset. A properly constructed CNN with correct training code should be able to memorize the answers to a small number of images quickly.</p>
<p>Construct a small dataset (e.g.&nbsp;just the images that you have collected). Then show that your model and training code is capable of memorizing the labels of this small data set.</p>
<p>With a large batch size (e.g.&nbsp;the entire small dataset) and learning rate that is not too high, You should be able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations).</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_wrapper(split_sets, convolution_dilation<span class="op">=</span><span class="dv">2</span>, epochs<span class="op">=</span><span class="dv">1000</span>, learning_rate<span class="op">=</span><span class="fl">0.0092</span>, momentum<span class="op">=</span><span class="fl">0.94</span>, wd<span class="op">=</span><span class="fl">1.6e-4</span>, batch_size<span class="op">=</span><span class="va">None</span>, use_cuda<span class="op">=</span><span class="va">True</span>, epoch_save_start<span class="op">=</span><span class="dv">100</span>, use_lr_sched<span class="op">=</span><span class="va">True</span>, epoch_slow_lr_start<span class="op">=</span><span class="dv">120</span>, slow_lr<span class="op">=</span><span class="fl">0.005</span>, low_momentum<span class="op">=</span><span class="fl">0.89</span>, print_every<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    train_batch_size <span class="op">=</span> <span class="bu">len</span>(split_sets[<span class="dv">0</span>]) <span class="cf">if</span> batch_size <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> batch_size</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    train_loader, val_loader, test_loader <span class="op">=</span> create_dataloaders(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        split_sets, batch_size<span class="op">=</span>train_batch_size, use_cuda<span class="op">=</span>use_cuda, shuffle<span class="op">=</span>shuffle)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># used for torch info summary</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        dev <span class="op">=</span> <span class="st">"cuda:0"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"CUDA unavailable, training on CPU"</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        dev <span class="op">=</span> <span class="st">"CPU"</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(dev)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    dummy_data <span class="op">=</span> torch.rand((train_batch_size, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)).to(device)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    trained_network, loss_dict <span class="op">=</span> train_loop(train_loader, val_loader, convolution_dilation<span class="op">=</span>convolution_dilation,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                                            epochs<span class="op">=</span>epochs, learning_rate<span class="op">=</span>learning_rate, momentum<span class="op">=</span>momentum, wd<span class="op">=</span>wd,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                                            batch_size<span class="op">=</span>train_batch_size, train_data<span class="op">=</span>dummy_data, use_cuda<span class="op">=</span>use_cuda,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>                                            epoch_save_start<span class="op">=</span>epoch_save_start, use_lr_sched<span class="op">=</span>use_lr_sched,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                                            epoch_slow_lr_start<span class="op">=</span>epoch_slow_lr_start, slow_lr<span class="op">=</span>slow_lr,</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>                                            low_momentum<span class="op">=</span>low_momentum, print_every<span class="op">=</span>print_every)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trained_network, loss_dict, (train_loader, val_loader, test_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>small_train, small_val, small_test, _ <span class="op">=</span> create_datasets(small_set_adjustment<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>small_split_sets <span class="op">=</span> (small_train, small_val, small_test)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(small_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>544</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>trained_network, loss_dict, loaders <span class="op">=</span> training_wrapper(small_split_sets, epochs<span class="op">=</span><span class="dv">200</span>, learning_rate<span class="op">=</span><span class="fl">0.00915</span>, momentum<span class="op">=</span><span class="fl">0.925</span>, wd<span class="op">=</span><span class="fl">1.6e-2</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [544, 9]                  --
├─Sequential: 1-1                        [544, 2, 14, 14]          --
│    └─Conv2d: 2-1                       [544, 12, 199, 199]       24,348
│    └─BatchNorm2d: 2-2                  [544, 12, 199, 199]       24
│    └─Mish: 2-3                         [544, 12, 199, 199]       --
│    └─AvgPool2d: 2-4                    [544, 12, 98, 98]         --
│    └─Mish: 2-5                         [544, 12, 98, 98]         --
│    └─Conv2d: 2-6                       [544, 8, 94, 94]          872
│    └─BatchNorm2d: 2-7                  [544, 8, 94, 94]          16
│    └─Mish: 2-8                         [544, 8, 94, 94]          --
│    └─MaxPool2d: 2-9                    [544, 8, 31, 31]          --
│    └─Mish: 2-10                        [544, 8, 31, 31]          --
│    └─Conv2d: 2-11                      [544, 4, 29, 29]          132
│    └─BatchNorm2d: 2-12                 [544, 4, 29, 29]          8
│    └─Mish: 2-13                        [544, 4, 29, 29]          --
│    └─Conv2d: 2-14                      [544, 2, 28, 28]          34
│    └─AvgPool2d: 2-15                   [544, 2, 14, 14]          --
│    └─Mish: 2-16                        [544, 2, 14, 14]          --
├─Flatten: 1-2                           [544, 392]                --
├─Dropout: 1-3                           [544, 392]                --
├─Linear: 1-4                            [544, 196]                77,028
├─Mish: 1-5                              [544, 196]                --
├─Dropout: 1-6                           [544, 196]                --
├─Linear: 1-7                            [544, 98]                 19,306
├─Mish: 1-8                              [544, 98]                 --
├─Dropout: 1-9                           [544, 98]                 --
├─Linear: 1-10                           [544, 98]                 9,702
├─Mish: 1-11                             [544, 98]                 --
├─Dropout: 1-12                          [544, 98]                 --
├─Linear: 1-13                           [544, 196]                19,404
├─Mish: 1-14                             [544, 196]                --
├─Dropout: 1-15                          [544, 196]                --
├─Linear: 1-16                           [544, 9]                  1,773
==========================================================================================
Total params: 152,647
Trainable params: 152,647
Non-trainable params: 0
Total mult-adds (G): 528.86
==========================================================================================
Input size (MB): 327.55
Forward/backward pass size (MB): 4790.22
Params size (MB): 0.61
Estimated Total Size (MB): 5118.38
==========================================================================================
Epoch 0 stats: train loss 2.1902267932891846, train acc 0.14154411852359772, val loss 2.209646701812744, val acc 0.06410256773233414
Epoch 10 stats: train loss 2.0526580810546875, train acc 0.27389705181121826, val loss 2.2041399478912354, val acc 0.07692307978868484
Epoch 20 stats: train loss 1.6772938966751099, train acc 0.4319852888584137, val loss 2.134267807006836, val acc 0.1794871836900711
Epoch 30 stats: train loss 1.0779668092727661, train acc 0.6397058963775635, val loss 1.6589462757110596, val acc 0.43589743971824646
Epoch 40 stats: train loss 0.7244219779968262, train acc 0.7591911554336548, val loss 1.2977230548858643, val acc 0.6025640964508057
Epoch 50 stats: train loss 0.5394009947776794, train acc 0.8363970518112183, val loss 1.4070969820022583, val acc 0.5769230723381042
Epoch 60 stats: train loss 0.29937633872032166, train acc 0.9227941036224365, val loss 1.5258115530014038, val acc 0.5897436141967773
Epoch 70 stats: train loss 0.16478298604488373, train acc 0.9650735259056091, val loss 1.097802996635437, val acc 0.6666666865348816
Epoch 80 stats: train loss 0.3268875777721405, train acc 0.875, val loss 10.344923973083496, val acc 0.3205128312110901
Epoch 90 stats: train loss 0.07267850637435913, train acc 0.9908088445663452, val loss 1.0189942121505737, val acc 0.7179487347602844
Epoch 100 stats: train loss 0.052319254726171494, train acc 0.9908088445663452, val loss 1.2930632829666138, val acc 0.6666666865348816
Epoch 110 stats: train loss 0.04485984519124031, train acc 0.9926470518112183, val loss 1.0033762454986572, val acc 0.7179487347602844
Epoch 120 stats: train loss 0.038824837654829025, train acc 0.9944853186607361, val loss 1.124265193939209, val acc 0.7051281929016113
Epoch 130 stats: train loss 0.03461975231766701, train acc 0.9944853186607361, val loss 1.0550730228424072, val acc 0.7051281929016113
Epoch 140 stats: train loss 0.02698923461139202, train acc 0.998161792755127, val loss 1.0080821514129639, val acc 0.7435897588729858
Epoch 150 stats: train loss 0.025783279910683632, train acc 0.998161792755127, val loss 0.9772793650627136, val acc 0.7564102411270142
Epoch 160 stats: train loss 0.021829893812537193, train acc 1.0, val loss 0.9808109998703003, val acc 0.7435897588729858
Epoch 170 stats: train loss 0.024946153163909912, train acc 0.998161792755127, val loss 0.9861094951629639, val acc 0.7435897588729858
Epoch 180 stats: train loss 0.029390716925263405, train acc 0.9963235259056091, val loss 0.9592354893684387, val acc 0.7692307829856873
Epoch 190 stats: train loss 0.018172232434153557, train acc 1.0, val loss 0.9660477042198181, val acc 0.7564102411270142</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As seen above, we are able to achieve 100% accuracy on the training set within 200 epochs.</p>
</section>
<section id="hyperparameter-search-10-pt" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-search-10-pt">3. Hyperparameter Search [10 pt]</h3>
</section>
<section id="part-a---1-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-a---1-pt">Part (a) - 1 pt</h3>
<p>List 3 hyperparameters that you think are most worth tuning. Choose at least one hyperparameter related to the model architecture.</p>
<p>I could try tuning the following hyperparameters:</p>
<ol type="1">
<li>The convolution dilation scale to tune how far apart the elements used in the convolution operation will be chosen. This would affect the latent representations, but choosing a good value would lead to better representations across the image, i.e., learn features far apart in the image that are informative to the ASL sign.</li>
<li>I could also tune the learning rate since in the overfit example it seems to converge slower than it needs to even though both the validation and train losses are virtually 0, and the model scored 100% accuracy on both sets respectively.</li>
<li>I could try tuning the weight decay parameter as it is essentially a <span class="math inline">\(L_2\)</span> penalty and can help the model converge to a local minima of the loss surface that is well regularized, and thus potentially perform better on the test set.</li>
</ol>
</section>
<section id="part-b---5-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-b---5-pt">Part (b) - 5 pt</h3>
<p>Tune the hyperparameters you listed in Part (a), trying as many values as you need to until you feel satisfied that you are getting a good model. Plot the training curve of at least 4 different hyperparameter settings.</p>
<p>I tested the hyperparameter changes I discussed above.</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_train_curves(loss_dict):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"</span><span class="sc">{</span>loss_dict[<span class="st">'config'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>([value.cpu().data.numpy() <span class="cf">for</span> value <span class="kw">in</span> loss_dict[<span class="st">"train_acc"</span>]])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">+</span><span class="dv">1</span>), [value.cpu().data.numpy() <span class="cf">for</span> value <span class="kw">in</span>  loss_dict[<span class="st">"train_acc"</span>]], label<span class="op">=</span><span class="st">"Train Accuracy"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">+</span><span class="dv">1</span>), [value.cpu().data.numpy() <span class="cf">for</span> value <span class="kw">in</span>  loss_dict[<span class="st">"val_acc"</span>]], label<span class="op">=</span><span class="st">"Validation Accuracy"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Accuracy"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_val_curves(loss_dict):</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"</span><span class="sc">{</span>loss_dict[<span class="st">'config'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>([value.cpu().data.numpy() <span class="cf">for</span> value <span class="kw">in</span> loss_dict[<span class="st">"val_loss"</span>]])</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">+</span><span class="dv">1</span>), [value.cpu().data.numpy() <span class="cf">for</span> value <span class="kw">in</span> loss_dict[<span class="st">"train_loss"</span>]], label<span class="op">=</span><span class="st">"Train Loss"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">+</span><span class="dv">1</span>), [value.cpu().data.numpy() <span class="cf">for</span> value <span class="kw">in</span> loss_dict[<span class="st">"val_loss"</span>]], label<span class="op">=</span><span class="st">"Validation Loss"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the default hyperparameters, we see the following graphs:</p>
<div class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plot_train_curves(loss_dict)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plot_val_curves(loss_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>My first experiment is changing the convolution dilation parameter to 4, which drastically increases the receptive field since distances between points on the kernel are now doubled. To help the optimizer explore the loss landscape more, I’ve also decreased the batch size, allowing for more variable estimates of the true gradient.</p>
<div class="cell" data-execution_count="137">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>trained_network_dilation_change, loss_dict_dilation, _ <span class="op">=</span> training_wrapper(split_sets, epochs<span class="op">=</span><span class="dv">200</span>, convolution_dilation<span class="op">=</span><span class="dv">6</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\ProgramData\Anaconda3\envs\pytorch-latest\lib\site-packages\torch\utils\data\dataset.py:342: UserWarning: Length of split at index 3 is 0. This might result in an empty dataset.
  warnings.warn(f"Length of split at index {i} is 0. "</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1554
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [256, 9]                  --
├─Sequential: 1-1                        [256, 2, 10, 10]          --
│    └─Conv2d: 2-1                       [256, 12, 199, 199]       24,348
│    └─BatchNorm2d: 2-2                  [256, 12, 199, 199]       24
│    └─Mish: 2-3                         [256, 12, 199, 199]       --
│    └─AvgPool2d: 2-4                    [256, 12, 98, 98]         --
│    └─Mish: 2-5                         [256, 12, 98, 98]         --
│    └─Conv2d: 2-6                       [256, 8, 86, 86]          872
│    └─BatchNorm2d: 2-7                  [256, 8, 86, 86]          16
│    └─Mish: 2-8                         [256, 8, 86, 86]          --
│    └─MaxPool2d: 2-9                    [256, 8, 28, 28]          --
│    └─Mish: 2-10                        [256, 8, 28, 28]          --
│    └─Conv2d: 2-11                      [256, 4, 22, 22]          132
│    └─BatchNorm2d: 2-12                 [256, 4, 22, 22]          8
│    └─Mish: 2-13                        [256, 4, 22, 22]          --
│    └─Conv2d: 2-14                      [256, 2, 21, 21]          34
│    └─AvgPool2d: 2-15                   [256, 2, 10, 10]          --
│    └─Mish: 2-16                        [256, 2, 10, 10]          --
├─Flatten: 1-2                           [256, 200]                --
├─Dropout: 1-3                           [256, 200]                --
├─Linear: 1-4                            [256, 100]                20,100
├─Mish: 1-5                              [256, 100]                --
├─Dropout: 1-6                           [256, 100]                --
├─Linear: 1-7                            [256, 50]                 5,050
├─Mish: 1-8                              [256, 50]                 --
├─Dropout: 1-9                           [256, 50]                 --
├─Linear: 1-10                           [256, 50]                 2,550
├─Mish: 1-11                             [256, 50]                 --
├─Dropout: 1-12                          [256, 50]                 --
├─Linear: 1-13                           [256, 100]                5,100
├─Mish: 1-14                             [256, 100]                --
├─Dropout: 1-15                          [256, 100]                --
├─Linear: 1-16                           [256, 9]                  909
==========================================================================================
Total params: 59,143
Trainable params: 59,143
Non-trainable params: 0
Total mult-adds (G): 248.52
==========================================================================================
Input size (MB): 154.14
Forward/backward pass size (MB): 2199.19
Params size (MB): 0.24
Estimated Total Size (MB): 2353.57
==========================================================================================
Epoch 0 stats: train loss 15.410856246948242, train acc 0.10296010226011276, val loss 2.208543300628662, val acc 0.09009009599685669
Epoch 10 stats: train loss 15.358366012573242, train acc 0.12226512283086777, val loss 2.2029340267181396, val acc 0.0855855867266655
Epoch 20 stats: train loss 14.124900817871094, train acc 0.2535392642021179, val loss 2.054001569747925, val acc 0.1621621698141098
Epoch 30 stats: train loss 9.969949722290039, train acc 0.49227800965309143, val loss 1.566210389137268, val acc 0.45495495200157166
Epoch 40 stats: train loss 7.062961578369141, train acc 0.6615186929702759, val loss 1.0672508478164673, val acc 0.639639675617218
Epoch 50 stats: train loss 4.820860862731934, train acc 0.7535392642021179, val loss 0.8818532824516296, val acc 0.7027027010917664
Epoch 60 stats: train loss 3.752593994140625, train acc 0.800514817237854, val loss 0.7705581188201904, val acc 0.7387387752532959
Epoch 70 stats: train loss 2.4686899185180664, train acc 0.8790218830108643, val loss 0.8243122100830078, val acc 0.7702702879905701
Epoch 80 stats: train loss 1.6108237504959106, train acc 0.931145429611206, val loss 0.8273671865463257, val acc 0.7792792916297913
Epoch 90 stats: train loss 0.7963055968284607, train acc 0.9646074771881104, val loss 1.102271318435669, val acc 0.7702702879905701
Epoch 100 stats: train loss 0.549174427986145, train acc 0.9736164808273315, val loss 1.0499085187911987, val acc 0.7612612843513489
Epoch 110 stats: train loss 0.5470301508903503, train acc 0.9813385009765625, val loss 1.1321996450424194, val acc 0.7837837934494019
Epoch 120 stats: train loss 0.3106619119644165, train acc 0.9897040128707886, val loss 1.1043915748596191, val acc 0.8063063025474548
Epoch 130 stats: train loss 0.3857925534248352, train acc 0.983912467956543, val loss 1.4123426675796509, val acc 0.7882882952690125
Epoch 140 stats: train loss 0.5862484574317932, train acc 0.9781209826469421, val loss 0.990760326385498, val acc 0.792792797088623
Epoch 150 stats: train loss 0.3087614178657532, train acc 0.9897040128707886, val loss 1.1066287755966187, val acc 0.8108108043670654
Epoch 160 stats: train loss 0.30867502093315125, train acc 0.9884170293807983, val loss 0.9956585764884949, val acc 0.7882882952690125
Epoch 170 stats: train loss 0.20069004595279694, train acc 0.9897040128707886, val loss 1.051531195640564, val acc 0.824324369430542
Epoch 180 stats: train loss 0.28945574164390564, train acc 0.9890605211257935, val loss 1.1408401727676392, val acc 0.792792797088623
Epoch 190 stats: train loss 0.3283458650112152, train acc 0.9826254844665527, val loss 1.1753385066986084, val acc 0.8063063025474548</code></pre>
</div>
</div>
<div class="cell" data-execution_count="138">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plot_train_curves(loss_dict_dilation)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plot_val_curves(loss_dict_dilation)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-14-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>My second experiment is the learning rate. It seems that so far, a good learning rate is roughly around 0.009. The tricky part is that it seems to need a good initial push (requiring a higher learning rate and momentum), but then needs a lower learning rate and momentum for the later epochs, otherwise it “bounces” around the loss landscape. I have implemented a rudimentary learning rate scheduler. It sets a lower learning rate and lower momentum after some number of epochs.</p>
<div class="cell" data-execution_count="143">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>trained_network_lr_change, loss_dict_lr, _ <span class="op">=</span> training_wrapper(split_sets, epochs<span class="op">=</span><span class="dv">200</span>, learning_rate <span class="op">=</span> <span class="fl">0.0092</span>, convolution_dilation<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">256</span>, slow_lr<span class="op">=</span><span class="fl">0.005</span>, low_momentum<span class="op">=</span><span class="fl">0.875</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\ProgramData\Anaconda3\envs\pytorch-latest\lib\site-packages\torch\utils\data\dataset.py:342: UserWarning: Length of split at index 3 is 0. This might result in an empty dataset.
  warnings.warn(f"Length of split at index {i} is 0. "</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1554
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [256, 9]                  --
├─Sequential: 1-1                        [256, 2, 14, 14]          --
│    └─Conv2d: 2-1                       [256, 12, 199, 199]       24,348
│    └─BatchNorm2d: 2-2                  [256, 12, 199, 199]       24
│    └─Mish: 2-3                         [256, 12, 199, 199]       --
│    └─AvgPool2d: 2-4                    [256, 12, 98, 98]         --
│    └─Mish: 2-5                         [256, 12, 98, 98]         --
│    └─Conv2d: 2-6                       [256, 8, 94, 94]          872
│    └─BatchNorm2d: 2-7                  [256, 8, 94, 94]          16
│    └─Mish: 2-8                         [256, 8, 94, 94]          --
│    └─MaxPool2d: 2-9                    [256, 8, 31, 31]          --
│    └─Mish: 2-10                        [256, 8, 31, 31]          --
│    └─Conv2d: 2-11                      [256, 4, 29, 29]          132
│    └─BatchNorm2d: 2-12                 [256, 4, 29, 29]          8
│    └─Mish: 2-13                        [256, 4, 29, 29]          --
│    └─Conv2d: 2-14                      [256, 2, 28, 28]          34
│    └─AvgPool2d: 2-15                   [256, 2, 14, 14]          --
│    └─Mish: 2-16                        [256, 2, 14, 14]          --
├─Flatten: 1-2                           [256, 392]                --
├─Dropout: 1-3                           [256, 392]                --
├─Linear: 1-4                            [256, 196]                77,028
├─Mish: 1-5                              [256, 196]                --
├─Dropout: 1-6                           [256, 196]                --
├─Linear: 1-7                            [256, 98]                 19,306
├─Mish: 1-8                              [256, 98]                 --
├─Dropout: 1-9                           [256, 98]                 --
├─Linear: 1-10                           [256, 98]                 9,702
├─Mish: 1-11                             [256, 98]                 --
├─Dropout: 1-12                          [256, 98]                 --
├─Linear: 1-13                           [256, 196]                19,404
├─Mish: 1-14                             [256, 196]                --
├─Dropout: 1-15                          [256, 196]                --
├─Linear: 1-16                           [256, 9]                  1,773
==========================================================================================
Total params: 152,647
Trainable params: 152,647
Non-trainable params: 0
Total mult-adds (G): 248.88
==========================================================================================
Input size (MB): 154.14
Forward/backward pass size (MB): 2254.22
Params size (MB): 0.61
Estimated Total Size (MB): 2408.97
==========================================================================================
Epoch 0 stats: train loss 15.392077445983887, train acc 0.11068211495876312, val loss 2.204162359237671, val acc 0.06306306272745132
Epoch 10 stats: train loss 14.260391235351562, train acc 0.31788933277130127, val loss 1.7526401281356812, val acc 0.36936938762664795
Epoch 20 stats: train loss 13.132797241210938, train acc 0.3133848309516907, val loss 2.06935715675354, val acc 0.21171171963214874
Epoch 30 stats: train loss 12.021281242370605, train acc 0.44015443325042725, val loss 1.792861819267273, val acc 0.4324324429035187
Epoch 40 stats: train loss 9.898470878601074, train acc 0.5405405759811401, val loss 1.5124071836471558, val acc 0.5180180072784424
Epoch 50 stats: train loss 15.038697242736816, train acc 0.18468469381332397, val loss 2.195452928543091, val acc 0.1666666716337204
Epoch 60 stats: train loss 8.527408599853516, train acc 0.6061776280403137, val loss 1.228888988494873, val acc 0.6531531810760498
Epoch 70 stats: train loss 7.589509010314941, train acc 0.6512226462364197, val loss 1.1624897718429565, val acc 0.6441441774368286
Epoch 80 stats: train loss 6.721262454986572, train acc 0.6904761791229248, val loss 1.0312669277191162, val acc 0.684684693813324
Epoch 90 stats: train loss 6.426058292388916, train acc 0.684684693813324, val loss 1.0359346866607666, val acc 0.6666666865348816
Epoch 100 stats: train loss 5.306894302368164, train acc 0.7471042275428772, val loss 0.9924043416976929, val acc 0.6531531810760498
Epoch 110 stats: train loss 4.711023330688477, train acc 0.7477477788925171, val loss 0.9397042989730835, val acc 0.7027027010917664
Epoch 120 stats: train loss 6.925260543823242, train acc 0.6634491682052612, val loss 1.0443801879882812, val acc 0.6756756901741028
Epoch 130 stats: train loss 3.8352866172790527, train acc 0.8037323355674744, val loss 0.9908797740936279, val acc 0.6666666865348816
Epoch 140 stats: train loss 3.8552117347717285, train acc 0.8288288116455078, val loss 0.9507279396057129, val acc 0.7162162065505981
Epoch 150 stats: train loss 3.080294132232666, train acc 0.8449163436889648, val loss 1.0254687070846558, val acc 0.7027027010917664
Epoch 160 stats: train loss 2.84724497795105, train acc 0.8687258958816528, val loss 1.0015403032302856, val acc 0.6891891956329346
Epoch 170 stats: train loss 2.051593780517578, train acc 0.8970398902893066, val loss 1.0477575063705444, val acc 0.7252252697944641
Epoch 180 stats: train loss 1.890684962272644, train acc 0.907979428768158, val loss 0.9463587403297424, val acc 0.7477477788925171
Epoch 190 stats: train loss 1.5381399393081665, train acc 0.9234234094619751, val loss 1.0599873065948486, val acc 0.7432432770729065</code></pre>
</div>
</div>
<div class="cell" data-execution_count="144">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plot_train_curves(loss_dict_lr)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plot_val_curves(loss_dict_lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-16-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>My third experiment is increasing the weight decay parameter since this could help us in finding a set of parameters for the model that corresponding to a well regularized model, which could help us mitigate possibly biases (e.g., inductive biases in making the model). There appear to be some overfitting towards the end (training accuracy is really high, while validation accuracy is still around 15% lower), so increasing the weight decay can help with regularizing the model.</p>
<div class="cell" data-execution_count="145">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>trained_network_wd_change, loss_dict_wd, _ <span class="op">=</span> training_wrapper(split_sets, epochs<span class="op">=</span><span class="dv">200</span>, wd<span class="op">=</span><span class="fl">4e-4</span>, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1554
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [256, 9]                  --
├─Sequential: 1-1                        [256, 2, 14, 14]          --
│    └─Conv2d: 2-1                       [256, 12, 199, 199]       24,348
│    └─BatchNorm2d: 2-2                  [256, 12, 199, 199]       24
│    └─Mish: 2-3                         [256, 12, 199, 199]       --
│    └─AvgPool2d: 2-4                    [256, 12, 98, 98]         --
│    └─Mish: 2-5                         [256, 12, 98, 98]         --
│    └─Conv2d: 2-6                       [256, 8, 94, 94]          872
│    └─BatchNorm2d: 2-7                  [256, 8, 94, 94]          16
│    └─Mish: 2-8                         [256, 8, 94, 94]          --
│    └─MaxPool2d: 2-9                    [256, 8, 31, 31]          --
│    └─Mish: 2-10                        [256, 8, 31, 31]          --
│    └─Conv2d: 2-11                      [256, 4, 29, 29]          132
│    └─BatchNorm2d: 2-12                 [256, 4, 29, 29]          8
│    └─Mish: 2-13                        [256, 4, 29, 29]          --
│    └─Conv2d: 2-14                      [256, 2, 28, 28]          34
│    └─AvgPool2d: 2-15                   [256, 2, 14, 14]          --
│    └─Mish: 2-16                        [256, 2, 14, 14]          --
├─Flatten: 1-2                           [256, 392]                --
├─Dropout: 1-3                           [256, 392]                --
├─Linear: 1-4                            [256, 196]                77,028
├─Mish: 1-5                              [256, 196]                --
├─Dropout: 1-6                           [256, 196]                --
├─Linear: 1-7                            [256, 98]                 19,306
├─Mish: 1-8                              [256, 98]                 --
├─Dropout: 1-9                           [256, 98]                 --
├─Linear: 1-10                           [256, 98]                 9,702
├─Mish: 1-11                             [256, 98]                 --
├─Dropout: 1-12                          [256, 98]                 --
├─Linear: 1-13                           [256, 196]                19,404
├─Mish: 1-14                             [256, 196]                --
├─Dropout: 1-15                          [256, 196]                --
├─Linear: 1-16                           [256, 9]                  1,773
==========================================================================================
Total params: 152,647
Trainable params: 152,647
Non-trainable params: 0
Total mult-adds (G): 248.88
==========================================================================================
Input size (MB): 154.14
Forward/backward pass size (MB): 2254.22
Params size (MB): 0.61
Estimated Total Size (MB): 2408.97
==========================================================================================
Epoch 0 stats: train loss 15.375836372375488, train acc 0.1190476194024086, val loss 2.197174549102783, val acc 0.10810811072587967
Epoch 10 stats: train loss 17.355548858642578, train acc 0.12162162363529205, val loss 2.203953742980957, val acc 0.11261261254549026
Epoch 20 stats: train loss 15.307247161865234, train acc 0.13256113231182098, val loss 2.205522298812866, val acc 0.11711712181568146
Epoch 30 stats: train loss 14.979476928710938, train acc 0.2142857164144516, val loss 2.17155385017395, val acc 0.19369369745254517
Epoch 40 stats: train loss 13.477688789367676, train acc 0.3429858386516571, val loss 1.9854182004928589, val acc 0.30630630254745483
Epoch 50 stats: train loss 11.441658020019531, train acc 0.47361648082733154, val loss 1.6328405141830444, val acc 0.4909909963607788
Epoch 60 stats: train loss 9.082146644592285, train acc 0.5765765905380249, val loss 1.3142846822738647, val acc 0.5495495796203613
Epoch 70 stats: train loss 7.835763454437256, train acc 0.6332046389579773, val loss 1.1715114116668701, val acc 0.5810810923576355
Epoch 80 stats: train loss 7.162685394287109, train acc 0.6486486792564392, val loss 1.0855984687805176, val acc 0.6576576828956604
Epoch 90 stats: train loss 6.6241841316223145, train acc 0.6821106672286987, val loss 1.0480635166168213, val acc 0.6486486792564392
Epoch 100 stats: train loss 6.159320831298828, train acc 0.6904761791229248, val loss 1.009700894355774, val acc 0.6711711883544922
Epoch 110 stats: train loss 5.926422595977783, train acc 0.7187902331352234, val loss 1.0666865110397339, val acc 0.662162184715271
Epoch 120 stats: train loss 5.472774028778076, train acc 0.7368082404136658, val loss 1.026951789855957, val acc 0.6306306719779968
Epoch 130 stats: train loss 5.53488302230835, train acc 0.739382266998291, val loss 0.9469945430755615, val acc 0.7027027010917664
Epoch 140 stats: train loss 5.256530284881592, train acc 0.7400257587432861, val loss 0.9621308445930481, val acc 0.6936936974525452
Epoch 150 stats: train loss 5.046815872192383, train acc 0.7548262476921082, val loss 0.9361984729766846, val acc 0.6936936974525452
Epoch 160 stats: train loss 5.8628387451171875, train acc 0.7471042275428772, val loss 0.9905980825424194, val acc 0.6531531810760498
Epoch 170 stats: train loss 4.965895652770996, train acc 0.7477477788925171, val loss 0.914722740650177, val acc 0.684684693813324
Epoch 180 stats: train loss 5.0589094161987305, train acc 0.7496782541275024, val loss 0.9293175339698792, val acc 0.6891891956329346
Epoch 190 stats: train loss 4.898757457733154, train acc 0.7612612843513489, val loss 0.9065548777580261, val acc 0.707207202911377</code></pre>
</div>
</div>
<div class="cell" data-execution_count="146">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plot_train_curves(loss_dict_wd)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plot_val_curves(loss_dict_wd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Finally, I will change the batch size to a much smaller batch size and combine all of these changes. Smaller batch sizes can help avoid overfitting as their gradients are computed more often from randomly sampled minibatches, and so the optimizer steps will be less homogenous (not always an evaluation of the gradient using the whole batch) with more of the weight decay effect (<a href="https://dl.acm.org/doi/pdf/10.5555/3327345.3327535">Li et al.</a>) and can therefore help generalization more.</p>
<div class="cell" data-execution_count="180">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>trained_network_all_change, loss_dict_all, _ <span class="op">=</span> training_wrapper(split_sets, epochs<span class="op">=</span><span class="dv">250</span>, wd<span class="op">=</span><span class="fl">8.2e-3</span>, batch_size<span class="op">=</span><span class="dv">128</span>, learning_rate <span class="op">=</span> <span class="fl">0.0091</span>, convolution_dilation<span class="op">=</span><span class="dv">4</span>, epoch_slow_lr_start<span class="op">=</span><span class="dv">40</span>, slow_lr<span class="op">=</span><span class="fl">0.0006</span>, low_momentum<span class="op">=</span><span class="fl">0.85</span>, print_every<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1554 222 443
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [128, 9]                  --
├─Sequential: 1-1                        [128, 2, 12, 12]          --
│    └─Conv2d: 2-1                       [128, 12, 199, 199]       24,348
│    └─BatchNorm2d: 2-2                  [128, 12, 199, 199]       24
│    └─Mish: 2-3                         [128, 12, 199, 199]       --
│    └─AvgPool2d: 2-4                    [128, 12, 98, 98]         --
│    └─Mish: 2-5                         [128, 12, 98, 98]         --
│    └─Conv2d: 2-6                       [128, 8, 90, 90]          872
│    └─BatchNorm2d: 2-7                  [128, 8, 90, 90]          16
│    └─Mish: 2-8                         [128, 8, 90, 90]          --
│    └─MaxPool2d: 2-9                    [128, 8, 30, 30]          --
│    └─Mish: 2-10                        [128, 8, 30, 30]          --
│    └─Conv2d: 2-11                      [128, 4, 26, 26]          132
│    └─BatchNorm2d: 2-12                 [128, 4, 26, 26]          8
│    └─Mish: 2-13                        [128, 4, 26, 26]          --
│    └─Conv2d: 2-14                      [128, 2, 25, 25]          34
│    └─AvgPool2d: 2-15                   [128, 2, 12, 12]          --
│    └─Mish: 2-16                        [128, 2, 12, 12]          --
├─Flatten: 1-2                           [128, 288]                --
├─Dropout: 1-3                           [128, 288]                --
├─Linear: 1-4                            [128, 144]                41,616
├─Mish: 1-5                              [128, 144]                --
├─Dropout: 1-6                           [128, 144]                --
├─Linear: 1-7                            [128, 72]                 10,440
├─Mish: 1-8                              [128, 72]                 --
├─Dropout: 1-9                           [128, 72]                 --
├─Linear: 1-10                           [128, 72]                 5,256
├─Mish: 1-11                             [128, 72]                 --
├─Dropout: 1-12                          [128, 72]                 --
├─Linear: 1-13                           [128, 144]                10,512
├─Mish: 1-14                             [128, 144]                --
├─Dropout: 1-15                          [128, 144]                --
├─Linear: 1-16                           [128, 9]                  1,305
==========================================================================================
Total params: 94,563
Trainable params: 94,563
Non-trainable params: 0
Total mult-adds (G): 124.35
==========================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 1113.21
Params size (MB): 0.38
Estimated Total Size (MB): 1190.66
==========================================================================================
Epoch 0 stats: train loss 28.5841064453125, train acc 0.11647361516952515, val loss 2.197230815887451, val acc 0.12162162363529205
Epoch 5 stats: train loss 28.08121109008789, train acc 0.17696267366409302, val loss 2.1167025566101074, val acc 0.22072072327136993
Epoch 10 stats: train loss 13.696343421936035, train acc 0.6370656490325928, val loss 1.0898371934890747, val acc 0.6531531810760498
Epoch 15 stats: train loss 7.898561000823975, train acc 0.7921493053436279, val loss 0.9454725980758667, val acc 0.7477477788925171
Epoch 20 stats: train loss 4.0260210037231445, train acc 0.8957529067993164, val loss 1.024498701095581, val acc 0.7612612843513489
Epoch 25 stats: train loss 1.8948074579238892, train acc 0.9523809552192688, val loss 1.0952352285385132, val acc 0.7747747898101807
Epoch 30 stats: train loss 1.3530230522155762, train acc 0.9697554707527161, val loss 1.1306430101394653, val acc 0.7747747898101807
Epoch 35 stats: train loss 0.8174170851707458, train acc 0.9858430027961731, val loss 1.2155072689056396, val acc 0.792792797088623
Epoch 40 stats: train loss 0.6318714022636414, train acc 0.9864864945411682, val loss 1.1263660192489624, val acc 0.7837837934494019
Epoch 45 stats: train loss 0.448335736989975, train acc 0.9987130165100098, val loss 0.9709597826004028, val acc 0.8018018007278442
Epoch 50 stats: train loss 0.5118060111999512, train acc 0.9954954981803894, val loss 0.9614671468734741, val acc 0.7747747898101807
Epoch 55 stats: train loss 0.5662422180175781, train acc 0.9954954981803894, val loss 0.9542304277420044, val acc 0.7792792916297913
Epoch 60 stats: train loss 0.7057164311408997, train acc 0.9942085146903992, val loss 0.9396142959594727, val acc 0.7837837934494019
Epoch 65 stats: train loss 0.7856013774871826, train acc 0.9916344881057739, val loss 0.9598453044891357, val acc 0.7882882952690125
Epoch 70 stats: train loss 0.8427558541297913, train acc 0.992277979850769, val loss 0.9707479476928711, val acc 0.7882882952690125
Epoch 75 stats: train loss 0.8486727476119995, train acc 0.9929215312004089, val loss 0.9790038466453552, val acc 0.7882882952690125
Epoch 80 stats: train loss 0.9017593860626221, train acc 0.9903475046157837, val loss 1.0021302700042725, val acc 0.7837837934494019
Epoch 85 stats: train loss 0.820013701915741, train acc 0.9948520064353943, val loss 0.9930695295333862, val acc 0.7837837934494019
Epoch 90 stats: train loss 0.9362152814865112, train acc 0.9929215312004089, val loss 0.9748563766479492, val acc 0.7837837934494019
Epoch 95 stats: train loss 0.8360600471496582, train acc 0.9897040128707886, val loss 0.9916162490844727, val acc 0.7792792916297913
Epoch 100 stats: train loss 0.8737056851387024, train acc 0.993565022945404, val loss 0.9981989860534668, val acc 0.792792797088623
Epoch 105 stats: train loss 0.8896049857139587, train acc 0.9890605211257935, val loss 0.9664173722267151, val acc 0.8018018007278442
Epoch 110 stats: train loss 0.8476203680038452, train acc 0.993565022945404, val loss 0.9546485543251038, val acc 0.7972972989082336
Epoch 115 stats: train loss 0.7963151931762695, train acc 0.993565022945404, val loss 0.9433620572090149, val acc 0.7972972989082336
Epoch 120 stats: train loss 0.9155631065368652, train acc 0.9909909963607788, val loss 0.908484697341919, val acc 0.8018018007278442
Epoch 125 stats: train loss 0.8317790031433105, train acc 0.992277979850769, val loss 0.8964632153511047, val acc 0.8018018007278442
Epoch 130 stats: train loss 0.7536457180976868, train acc 0.9954954981803894, val loss 0.8778294324874878, val acc 0.8063063025474548
Epoch 135 stats: train loss 0.8332047462463379, train acc 0.9916344881057739, val loss 0.8602469563484192, val acc 0.8108108043670654
Epoch 140 stats: train loss 0.8106679916381836, train acc 0.9948520064353943, val loss 0.8562514185905457, val acc 0.8108108043670654
Epoch 145 stats: train loss 0.8192512392997742, train acc 0.9929215312004089, val loss 0.8472617864608765, val acc 0.8108108043670654
Epoch 150 stats: train loss 0.8533034920692444, train acc 0.9909909963607788, val loss 0.8444656729698181, val acc 0.815315306186676
Epoch 155 stats: train loss 0.7061498761177063, train acc 0.9948520064353943, val loss 0.8536790013313293, val acc 0.8198198676109314
Epoch 160 stats: train loss 0.7325777411460876, train acc 0.993565022945404, val loss 0.8460732102394104, val acc 0.824324369430542
Epoch 165 stats: train loss 0.6764543652534485, train acc 0.9961389899253845, val loss 0.8461742997169495, val acc 0.824324369430542
Epoch 170 stats: train loss 0.6843701004981995, train acc 0.9954954981803894, val loss 0.8520249128341675, val acc 0.8288288712501526
Epoch 175 stats: train loss 0.6204771995544434, train acc 0.9974260330200195, val loss 0.8570889830589294, val acc 0.8288288712501526
Epoch 180 stats: train loss 0.6215803027153015, train acc 0.9948520064353943, val loss 0.8668225407600403, val acc 0.8198198676109314
Epoch 185 stats: train loss 0.6371103525161743, train acc 0.9916344881057739, val loss 0.8665729761123657, val acc 0.824324369430542
Epoch 190 stats: train loss 0.5834133625030518, train acc 0.9961389899253845, val loss 0.861064612865448, val acc 0.8198198676109314
Epoch 195 stats: train loss 0.6988312602043152, train acc 0.9909909963607788, val loss 0.8686079978942871, val acc 0.8198198676109314
Epoch 200 stats: train loss 0.618105947971344, train acc 0.9942085146903992, val loss 0.8954548835754395, val acc 0.7972972989082336
Epoch 205 stats: train loss 0.7293776273727417, train acc 0.9903475046157837, val loss 0.9058334827423096, val acc 0.7972972989082336
Epoch 210 stats: train loss 0.7890384793281555, train acc 0.9909909963607788, val loss 0.9388367533683777, val acc 0.7972972989082336
Epoch 215 stats: train loss 0.7962871193885803, train acc 0.9916344881057739, val loss 0.9520527720451355, val acc 0.8063063025474548
Epoch 220 stats: train loss 0.7987181544303894, train acc 0.992277979850769, val loss 1.041548252105713, val acc 0.7882882952690125
Epoch 225 stats: train loss 0.7861189842224121, train acc 0.9929215312004089, val loss 0.9843763709068298, val acc 0.8018018007278442
Epoch 230 stats: train loss 0.8056790232658386, train acc 0.993565022945404, val loss 0.9641956090927124, val acc 0.8018018007278442
Epoch 235 stats: train loss 0.8469924926757812, train acc 0.9942085146903992, val loss 1.0313202142715454, val acc 0.792792797088623
Epoch 240 stats: train loss 0.774444043636322, train acc 0.9942085146903992, val loss 1.0033396482467651, val acc 0.792792797088623
Epoch 245 stats: train loss 0.6848770976066589, train acc 0.9948520064353943, val loss 0.9828683137893677, val acc 0.7882882952690125</code></pre>
</div>
</div>
<div class="cell" data-execution_count="181">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plot_train_curves(loss_dict_all)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plot_val_curves(loss_dict_all)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-20-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="part-c---2-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-c---2-pt">Part (c) - 2 pt</h3>
<p>Choose the best model out of all the ones that you have trained. Justify your choice.</p>
<p>I choose my best model to be the combined model that has changes to all of the tested hyperparameters primarily because it has the best validation performance, i.e., <span class="math inline">\(\approx 87\%\)</span> accuracy on the validation set, whereas the best validation accuracies from the other models were approximately 75%. This model in particular used a smaller minibatch size, meaning that it was able to get more hetergeneous estimates of the true gradient. As such, it likely explored the loss surface more, and perhaps it has a more reliable set of parameters. In particular, large minibatch sizes (<code>batch_size</code> &gt; 512) appear to converge to sharp minima according to <a href="https://arxiv.org/pdf/1609.04836.pdf">Keskar et al.</a>, which can lead to worse generalization performance.</p>
</section>
<section id="part-d---2-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-d---2-pt">Part (d) - 2 pt</h3>
<p>Report the test accuracy of your best model. You should only do this step once and prior to this step you should have only used the training and validation data.</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="63">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>model_state_path <span class="op">=</span> <span class="st">"./models/valacc_0.8333333730697632-convdial_4-lr_0.0091-momentum_0.94-batch_size_128-epoch_num_174.mdlckpt"</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>weight_loaded_model <span class="op">=</span> SimpleCNN(conv_dilation<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>weight_loaded_model.load_state_dict(torch.load(model_state_path))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>_, _, test_loader <span class="op">=</span> create_dataloaders(</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    split_sets, use_cuda<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>test_acc, test_correct, test_total, test_loss <span class="op">=</span> compute_test_performance(weight_loaded_model, test_loader)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_acc, test_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9548532731376975 0.276869535446167</code></pre>
</div>
</div>
<p>The test accuracy is <span class="math inline">\(\approx 95\%\)</span> (94.808%), so the model learned the data extremely well and is a very good classifier for the A to I ASL signs. Further, the model surprisingly performed better on the test set than the validation set.</p>
</section>
<section id="transfer-learning-15-pt" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning-15-pt">4. Transfer Learning [15 pt]</h3>
<p>For many image classification tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data.</p>
<p>One of the better options is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed <strong>Transfer Learning</strong>. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.</p>
<p>As you have learned from the CNN lecture, convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an ImageNet pre-trained AlexNet model to extract features in this assignment.</p>
</section>
<section id="part-a---5-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-a---5-pt">Part (a) - 5 pt</h3>
<p>Here is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch will download the pretrained weights from the internet.</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> torchvision.models.alexnet(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> alexnet.to(device<span class="op">=</span><span class="st">"cuda:0"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\ProgramData\Anaconda3\envs\pytorch-latest\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
c:\ProgramData\Anaconda3\envs\pytorch-latest\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)</code></pre>
</div>
</div>
<p>The alexnet model is split up into two components: <em>alexnet.features</em> and <em>alexnet.classifier</em>. The first neural network component, <em>alexnet.features</em>, is used to compute convolutional features, which are taken as input in <em>alexnet.classifier</em>.</p>
<p>The neural network alexnet.features expects an image tensor of shape Nx3x224x224 as input and it will output a tensor of shape Nx256x6x6 . (N = batch size).</p>
<p>Compute the AlexNet features for each of your training, validation, and test data. Here is an example code snippet showing how you can compute the AlexNet features for some images (your actual code might be different):</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># img = ... a PyTorch tensor with shape [N,3,224,224] containing hand images ...</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>train_loader, val_loader, test_loader <span class="op">=</span> create_dataloaders(split_sets, batch_size<span class="op">=</span><span class="bu">len</span>(train_set))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>train_embeds, val_embeds, test_embeds <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>train_targets, val_targets, test_targets <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span>, targets <span class="op">=</span> batch</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    train_embeds <span class="op">=</span> alexnet.features(<span class="bu">input</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    train_targets <span class="op">=</span> targets</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(val_loader):</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span>, targets <span class="op">=</span> batch</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    val_embeds <span class="op">=</span> alexnet.features(<span class="bu">input</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    val_targets <span class="op">=</span> targets</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(test_loader):</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span>, targets <span class="op">=</span> batch</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    test_embeds <span class="op">=</span> alexnet.features(<span class="bu">input</span>)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    test_targets <span class="op">=</span> targets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>train_embeds.size(), val_embeds.size(), test_embeds.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(torch.Size([1554, 256, 6, 6]),
 torch.Size([222, 256, 6, 6]),
 torch.Size([443, 256, 6, 6]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>torch.save(train_embeds, <span class="st">"./train_embeds"</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>torch.save(val_embeds, <span class="st">"./val_embeds"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>torch.save(test_embeds, <span class="st">"./test_embeds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>train_embeds <span class="op">=</span> torch.load(<span class="st">"./train_embeds"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>val_embeds <span class="op">=</span> torch.load(<span class="st">"./val_embeds"</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>test_embeds <span class="op">=</span> torch.load(<span class="st">"./test_embeds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Save the computed features</strong>. You will be using these features as input to your neural network in Part (b), and you do not want to re-compute the features every time. Instead, run <em>alexnet.features</em> once for each image, and save the result.</p>
</section>
<section id="part-b---3-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-b---3-pt">Part (b) - 3 pt</h3>
<p>Build a convolutional neural network model that takes as input these AlexNet features, and makes a prediction. Your model should be a subclass of nn.Module.</p>
<p>Explain your choice of neural network architecture: how many layers did you choose? What types of layers did you use: fully-connected or convolutional? What about other decisions like pooling layers, activation functions, number of channels / hidden units in each layer?</p>
<p>Here is an example of how your model may be called:</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="12">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransferCNN(nn.Module):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, conv_dilation<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_embeddings <span class="op">=</span> nn.Sequential(</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">4</span>, dilation<span class="op">=</span>conv_dilation),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">5</span>),</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">2</span>),</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">5</span>),</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">4</span>, dilation<span class="op">=</span><span class="dv">2</span><span class="op">*</span>conv_dilation),</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">2</span>),</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">4</span>, dilation<span class="op">=</span>conv_dilation),</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>            nn.Mish(),</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, <span class="dv">64</span>, <span class="dv">4</span>),</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>            nn.Mish())</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># programmatically get feature embedding size </span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._feature_embed_size <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.feature_embeddings(torch.rand(<span class="dv">1</span>, <span class="dv">256</span>, <span class="dv">6</span>, <span class="dv">6</span>)).detach().size())</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding_size <span class="op">=</span> np.prod(<span class="va">self</span>._feature_embed_size)</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use dropout to regularize model </span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># autoencoder like architecture - just remove reconstruction layers</span></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act1 <span class="op">=</span> nn.Mish()</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>)</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act2 <span class="op">=</span> nn.Mish()</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>)</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act3 <span class="op">=</span> nn.Mish()</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">4</span>, <span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act4 <span class="op">=</span> nn.Mish()</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc5 <span class="op">=</span> nn.Linear(<span class="va">self</span>.embedding_size<span class="op">//</span><span class="dv">2</span>, <span class="dv">9</span>)</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create latent space rep.</span></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.flatten(<span class="va">self</span>.feature_embeddings(<span class="bu">input</span>))</span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.act1(<span class="va">self</span>.fc1(z))</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.dropout(z)</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.act2(<span class="va">self</span>.fc2(z))</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.dropout(z)</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.act3(<span class="va">self</span>.fc3(z))</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.dropout(z)</span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.act4(<span class="va">self</span>.fc4(z))</span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.dropout(z)</span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc5(z)</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TransferCNN().to(<span class="st">"cuda:0"</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(model, (1, 256, 6, 6), verbose=1)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(train_embeds)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> F.softmax(output)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>torch.<span class="bu">max</span>(prob, <span class="dv">1</span>)[<span class="dv">1</span>].detach()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\ericz\AppData\Local\Temp\ipykernel_24132\3956553545.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  prob = F.softmax(output)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor([7, 7, 7,  ..., 7, 7, 7], device='cuda:0')</code></pre>
</div>
</div>
<p>I decided to still use a fairly deep network for this since I wanted the “fine tuning” to be substantial since AlexNet was trained on ImageNet, and ImageNet is a rather general image database. More specifically, the AlexNet representations are rather compressed (a <span class="math inline">\(6 \times 6\)</span> matrix per feature map, i.e., convolution output), and so I wanted to learn some additional less compressed representations. Thus, over the course of some layers, I aimed to expand the size of each feature map while moderately cutting down on the number of output channel for each feature map. I also used batch norms to help keep the scales of each convolution output to be normalized, while also using dilated convolutions to keep the model from focusing too much on local features, which may be a problem due to the compressed nature of the AlexNet feature representations (maps). I only used max pooling because I wanted the model to focus on the most important features since some of the uncompressed features learned from the really compressed representation may be meaningless to average over, and also I wanted to sort of compare average pooling to max pooling. Then for the linear layers, I once again used a similar autoencoder like architecture since it allows for the “distilling” of latent representations. This is similar to the logic of the max pooling I used in the convoluion layers. I used the <code>Mish</code> activation function for the same reasons I used it in the <code>SimpleCNN</code> model.</p>
</section>
<section id="part-c---5-pt" class="level3">
<h3 class="anchored" data-anchor-id="part-c---5-pt">Part (c) - 5 pt</h3>
<p>Train your new network, including any hyperparameter tuning. Plot and submit the training curve of your best model only.</p>
<p>Note: Depending on how you are caching (saving) your AlexNet features, PyTorch might still be tracking updates to the <strong>AlexNet weights</strong>, which we are not tuning. One workaround is to convert your AlexNet feature tensor into a numpy array, and then back into a PyTorch tensor.</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="13">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_loss(preds, targets):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> criterion(preds, targets)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transfer_train_loop(train_data, val_data, convolution_dilation<span class="op">=</span><span class="dv">2</span>, epochs<span class="op">=</span><span class="dv">1000</span>, learning_rate<span class="op">=</span><span class="fl">0.0095</span>, momentum<span class="op">=</span><span class="fl">0.95</span>, wd<span class="op">=</span><span class="fl">1e-4</span>, batch_size<span class="op">=</span><span class="va">None</span>, use_cuda<span class="op">=</span><span class="va">True</span>, use_tqdm<span class="op">=</span><span class="va">False</span>, epoch_save_start<span class="op">=</span><span class="dv">100</span>, use_lr_sched<span class="op">=</span><span class="va">True</span>, epoch_slow_lr_start<span class="op">=</span><span class="dv">120</span>, slow_lr<span class="op">=</span><span class="fl">0.00001</span>, low_momentum<span class="op">=</span><span class="fl">0.8</span>, print_every<span class="op">=</span><span class="dv">5</span>, model_path_prefix<span class="op">=</span><span class="st">"./models/transfer/"</span>):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    torch.set_float32_matmul_precision <span class="op">=</span> <span class="st">"high"</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>        dev <span class="op">=</span> <span class="st">"cuda:0"</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"CUDA unavailable, training on CPU"</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>        dev <span class="op">=</span> <span class="st">"CPU"</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(dev)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    network <span class="op">=</span> TransferCNN(conv_dilation<span class="op">=</span>convolution_dilation)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    network <span class="op">=</span> network.to(device)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(network.parameters(</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    ), lr<span class="op">=</span>learning_rate, weight_decay<span class="op">=</span>wd)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use for Nvidia AMP training cycle</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> torch.cuda.amp.GradScaler()</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train_data <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        summary(network, input_data<span class="op">=</span>train_data[<span class="dv">0</span>], verbose<span class="op">=</span><span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>    loss_dict <span class="op">=</span> {<span class="st">"config"</span>: <span class="ss">f"Convolution Dilation: </span><span class="sc">{</span>convolution_dilation<span class="sc">}</span><span class="ss">, Epochs: </span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">, Lr: </span><span class="sc">{</span>learning_rate<span class="sc">}</span><span class="ss">, Momentum:</span><span class="sc">{</span>momentum<span class="sc">}</span><span class="ss">, Weight Decay: </span><span class="sc">{</span>wd<span class="sc">}</span><span class="ss">, Batch Size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"train_loss"</span>: [], <span class="st">"val_loss"</span>: [],</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"train_acc"</span>: [], <span class="st">"val_acc"</span>: []}</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># counter for determining checkpoints</span></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    best_val_acc <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run_epoch():</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>        epoch_loss, val_loss <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>        train_correct, train_total <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>        val_correct, val_total <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reenable train mode to enable dropout</span></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>        network.train()</span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>        inputs, targets <span class="op">=</span> train_data</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> inputs.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> targets.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>        network.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16): </span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> network(inputs)</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> compute_loss(preds, targets)</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>        scaler.scale(loss).backward()</span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>        scaler.step(optimizer)</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>        scaler.update()</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># eval mode to disable dropout</span></span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>            network.<span class="bu">eval</span>()</span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>            epoch_loss <span class="op">+=</span> loss</span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a>            _, category_preds <span class="op">=</span> torch.<span class="bu">max</span>(preds, <span class="dv">1</span>)</span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>            train_correct <span class="op">+=</span> (category_preds <span class="op">==</span></span>
<span id="cb42-61"><a href="#cb42-61" aria-hidden="true" tabindex="-1"></a>                                    targets).<span class="bu">sum</span>()</span>
<span id="cb42-62"><a href="#cb42-62" aria-hidden="true" tabindex="-1"></a>            train_total <span class="op">+=</span> preds.size()[<span class="dv">0</span>]</span>
<span id="cb42-63"><a href="#cb42-63" aria-hidden="true" tabindex="-1"></a>            loss_dict[<span class="st">"train_loss"</span>].append(loss)</span>
<span id="cb42-64"><a href="#cb42-64" aria-hidden="true" tabindex="-1"></a>            loss_dict[<span class="st">"train_acc"</span>].append(train_correct<span class="op">/</span>train_total)</span>
<span id="cb42-65"><a href="#cb42-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-66"><a href="#cb42-66" aria-hidden="true" tabindex="-1"></a>            inputs, targets <span class="op">=</span> val_data</span>
<span id="cb42-67"><a href="#cb42-67" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> inputs.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-68"><a href="#cb42-68" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> targets.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-69"><a href="#cb42-69" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16): </span>
<span id="cb42-70"><a href="#cb42-70" aria-hidden="true" tabindex="-1"></a>                preds <span class="op">=</span> network(inputs)</span>
<span id="cb42-71"><a href="#cb42-71" aria-hidden="true" tabindex="-1"></a>                batch_val_loss <span class="op">=</span> compute_loss(preds, targets)</span>
<span id="cb42-72"><a href="#cb42-72" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">+=</span> batch_val_loss</span>
<span id="cb42-73"><a href="#cb42-73" aria-hidden="true" tabindex="-1"></a>            _, category_preds <span class="op">=</span> torch.<span class="bu">max</span>(preds, <span class="dv">1</span>)</span>
<span id="cb42-74"><a href="#cb42-74" aria-hidden="true" tabindex="-1"></a>            val_correct <span class="op">+=</span> (category_preds <span class="op">==</span></span>
<span id="cb42-75"><a href="#cb42-75" aria-hidden="true" tabindex="-1"></a>                                    targets).<span class="bu">sum</span>()</span>
<span id="cb42-76"><a href="#cb42-76" aria-hidden="true" tabindex="-1"></a>            val_total <span class="op">+=</span> preds.size()[<span class="dv">0</span>]</span>
<span id="cb42-77"><a href="#cb42-77" aria-hidden="true" tabindex="-1"></a>            loss_dict[<span class="st">"val_loss"</span>].append(batch_val_loss)</span>
<span id="cb42-78"><a href="#cb42-78" aria-hidden="true" tabindex="-1"></a>            loss_dict[<span class="st">"val_acc"</span>].append(val_correct<span class="op">/</span>val_total)</span>
<span id="cb42-79"><a href="#cb42-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> epoch_loss, train_correct, train_total, val_loss, val_correct, val_total</span>
<span id="cb42-80"><a href="#cb42-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-81"><a href="#cb42-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the fancy TQDM progress bar slows down model training likely due to GPU -&gt; CPU copies</span></span>
<span id="cb42-82"><a href="#cb42-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_tqdm:</span>
<span id="cb42-83"><a href="#cb42-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> trange(epochs, desc<span class="op">=</span><span class="st">"Train epochs"</span>, unit<span class="op">=</span><span class="st">"epoch"</span>) <span class="im">as</span> train_bar:</span>
<span id="cb42-84"><a href="#cb42-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> epoch <span class="kw">in</span> train_bar:</span>
<span id="cb42-85"><a href="#cb42-85" aria-hidden="true" tabindex="-1"></a>                epoch_loss, train_correct, train_total, val_loss, val_correct, val_total <span class="op">=</span> run_epoch()</span>
<span id="cb42-86"><a href="#cb42-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-87"><a href="#cb42-87" aria-hidden="true" tabindex="-1"></a>                <span class="co"># rudimentary learning rate scheduler</span></span>
<span id="cb42-88"><a href="#cb42-88" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> use_lr_sched <span class="kw">and</span> epoch <span class="op">&gt;=</span> epoch_slow_lr_start:</span>
<span id="cb42-89"><a href="#cb42-89" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> param <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb42-90"><a href="#cb42-90" aria-hidden="true" tabindex="-1"></a>                        param[<span class="st">"lr"</span>] <span class="op">=</span> slow_lr</span>
<span id="cb42-91"><a href="#cb42-91" aria-hidden="true" tabindex="-1"></a>                        param[<span class="st">"momentum"</span>] <span class="op">=</span> low_momentum</span>
<span id="cb42-92"><a href="#cb42-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-93"><a href="#cb42-93" aria-hidden="true" tabindex="-1"></a>                <span class="co"># model check point based on validation accuracy</span></span>
<span id="cb42-94"><a href="#cb42-94" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> torch.<span class="bu">round</span>(val_correct<span class="op">/</span>val_total, decimals<span class="op">=</span><span class="dv">3</span>) <span class="op">&gt;</span> best_val_acc <span class="kw">and</span> epoch <span class="op">&gt;=</span> epoch_save_start:</span>
<span id="cb42-95"><a href="#cb42-95" aria-hidden="true" tabindex="-1"></a>                    torch.save(network.state_dict(</span>
<span id="cb42-96"><a href="#cb42-96" aria-hidden="true" tabindex="-1"></a>                    ), model_path_prefix <span class="op">+</span> <span class="ss">f"valacc_</span><span class="sc">{</span>val_correct<span class="op">/</span>val_total<span class="sc">}</span><span class="ss">-convdial_</span><span class="sc">{</span>convolution_dilation<span class="sc">}</span><span class="ss">-lr_</span><span class="sc">{</span>learning_rate<span class="sc">}</span><span class="ss">-momentum_</span><span class="sc">{</span>momentum<span class="sc">}</span><span class="ss">-batch_size_</span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">-epoch_num_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.mdlckpt"</span>)</span>
<span id="cb42-97"><a href="#cb42-97" aria-hidden="true" tabindex="-1"></a>                    best_val_acc <span class="op">=</span> torch.<span class="bu">round</span>(val_correct<span class="op">/</span>val_total, decimals<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb42-98"><a href="#cb42-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-99"><a href="#cb42-99" aria-hidden="true" tabindex="-1"></a>                train_bar.set_postfix(epoch_loss<span class="op">=</span>(epoch_loss).item(),</span>
<span id="cb42-100"><a href="#cb42-100" aria-hidden="true" tabindex="-1"></a>                                      train_acc<span class="op">=</span>(train_correct<span class="op">/</span>train_total).item(),</span>
<span id="cb42-101"><a href="#cb42-101" aria-hidden="true" tabindex="-1"></a>                                      train_correct<span class="op">=</span>train_correct.item(),</span>
<span id="cb42-102"><a href="#cb42-102" aria-hidden="true" tabindex="-1"></a>                                      train_total<span class="op">=</span>train_total,</span>
<span id="cb42-103"><a href="#cb42-103" aria-hidden="true" tabindex="-1"></a>                                      val_loss<span class="op">=</span>val_loss.item(),</span>
<span id="cb42-104"><a href="#cb42-104" aria-hidden="true" tabindex="-1"></a>                                      val_acc<span class="op">=</span>(val_correct<span class="op">/</span>val_total).item())</span>
<span id="cb42-105"><a href="#cb42-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb42-106"><a href="#cb42-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb42-107"><a href="#cb42-107" aria-hidden="true" tabindex="-1"></a>            <span class="co"># run the epoch</span></span>
<span id="cb42-108"><a href="#cb42-108" aria-hidden="true" tabindex="-1"></a>            epoch_loss, train_correct, train_total, val_loss, val_correct, val_total <span class="op">=</span> run_epoch()</span>
<span id="cb42-109"><a href="#cb42-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-110"><a href="#cb42-110" aria-hidden="true" tabindex="-1"></a>            <span class="co"># rudimentary learning rate scheduler</span></span>
<span id="cb42-111"><a href="#cb42-111" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> use_lr_sched <span class="kw">and</span> epoch <span class="op">&gt;=</span> epoch_slow_lr_start:</span>
<span id="cb42-112"><a href="#cb42-112" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> param <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb42-113"><a href="#cb42-113" aria-hidden="true" tabindex="-1"></a>                    param[<span class="st">"lr"</span>] <span class="op">=</span> slow_lr</span>
<span id="cb42-114"><a href="#cb42-114" aria-hidden="true" tabindex="-1"></a>                    param[<span class="st">"momentum"</span>] <span class="op">=</span> low_momentum</span>
<span id="cb42-115"><a href="#cb42-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-116"><a href="#cb42-116" aria-hidden="true" tabindex="-1"></a>            <span class="co"># model check point based on validation accuracy</span></span>
<span id="cb42-117"><a href="#cb42-117" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> torch.<span class="bu">round</span>(val_correct<span class="op">/</span>val_total, decimals<span class="op">=</span><span class="dv">3</span>) <span class="op">&gt;</span> best_val_acc <span class="kw">and</span> epoch <span class="op">&gt;=</span> epoch_save_start:</span>
<span id="cb42-118"><a href="#cb42-118" aria-hidden="true" tabindex="-1"></a>                torch.save(network.state_dict(</span>
<span id="cb42-119"><a href="#cb42-119" aria-hidden="true" tabindex="-1"></a>                ), model_path_prefix <span class="op">+</span> <span class="ss">f"valacc_</span><span class="sc">{</span>val_correct<span class="op">/</span>val_total<span class="sc">}</span><span class="ss">-convdial_</span><span class="sc">{</span>convolution_dilation<span class="sc">}</span><span class="ss">-lr_</span><span class="sc">{</span>learning_rate<span class="sc">}</span><span class="ss">-momentum_</span><span class="sc">{</span>momentum<span class="sc">}</span><span class="ss">-batch_size_</span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">-epoch_num_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.mdlckpt"</span>)</span>
<span id="cb42-120"><a href="#cb42-120" aria-hidden="true" tabindex="-1"></a>                best_val_acc <span class="op">=</span> torch.<span class="bu">round</span>(val_correct<span class="op">/</span>val_total, decimals<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb42-121"><a href="#cb42-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-122"><a href="#cb42-122" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> print_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb42-123"><a href="#cb42-123" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> stats: train loss </span><span class="sc">{</span>epoch_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, train acc </span><span class="sc">{</span>(train_correct<span class="op">/</span>train_total)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, val loss </span><span class="sc">{</span>val_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, val acc </span><span class="sc">{</span>(val_correct<span class="op">/</span>val_total)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-124"><a href="#cb42-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-125"><a href="#cb42-125" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> network, loss_dict</span>
<span id="cb42-126"><a href="#cb42-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-127"><a href="#cb42-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-128"><a href="#cb42-128" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_transfer_test_performance(network, test_data):</span>
<span id="cb42-129"><a href="#cb42-129" aria-hidden="true" tabindex="-1"></a>    test_correct, test_total <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb42-130"><a href="#cb42-130" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb42-131"><a href="#cb42-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-132"><a href="#cb42-132" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="bu">next</span>(network.parameters()).device</span>
<span id="cb42-133"><a href="#cb42-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-134"><a href="#cb42-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># disable dropout for test time</span></span>
<span id="cb42-135"><a href="#cb42-135" aria-hidden="true" tabindex="-1"></a>        network.<span class="bu">eval</span>()</span>
<span id="cb42-136"><a href="#cb42-136" aria-hidden="true" tabindex="-1"></a>        inputs, targets <span class="op">=</span> test_data</span>
<span id="cb42-137"><a href="#cb42-137" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> inputs.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-138"><a href="#cb42-138" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> targets.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-139"><a href="#cb42-139" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> network(inputs)</span>
<span id="cb42-140"><a href="#cb42-140" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">+=</span> compute_loss(preds, targets)</span>
<span id="cb42-141"><a href="#cb42-141" aria-hidden="true" tabindex="-1"></a>        _, category_preds <span class="op">=</span> torch.<span class="bu">max</span>(preds, <span class="dv">1</span>)</span>
<span id="cb42-142"><a href="#cb42-142" aria-hidden="true" tabindex="-1"></a>        test_correct <span class="op">+=</span> <span class="bu">float</span>((category_preds <span class="op">==</span> targets).<span class="bu">sum</span>())</span>
<span id="cb42-143"><a href="#cb42-143" aria-hidden="true" tabindex="-1"></a>        test_total <span class="op">+=</span> <span class="bu">len</span>(preds)</span>
<span id="cb42-144"><a href="#cb42-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_correct<span class="op">/</span>test_total, test_correct, test_total, test_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>train_embeds, val_embeds <span class="op">=</span> torch.tensor(train_embeds.detach()), torch.tensor(val_embeds.detach())</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>transfer_model, loss_dict_transfer <span class="op">=</span> transfer_train_loop((train_embeds, train_targets), (val_embeds, val_targets), learning_rate<span class="op">=</span><span class="fl">0.00001</span>, use_tqdm<span class="op">=</span><span class="va">True</span>, epoch_save_start<span class="op">=</span><span class="dv">300</span>, epoch_slow_lr_start<span class="op">=</span><span class="dv">300</span>, slow_lr<span class="op">=</span> <span class="fl">0.000005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\ericz\AppData\Local\Temp\ipykernel_25000\2895223788.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_embeds, val_embeds = torch.tensor(train_embeds.detach()), torch.tensor(val_embeds.detach())</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TransferCNN                              [1554, 9]                 --
├─Sequential: 1-1                        [1554, 64, 3, 3]          --
│    └─ConvTranspose2d: 2-1              [1554, 128, 12, 12]       524,416
│    └─Mish: 2-2                         [1554, 128, 12, 12]       --
│    └─ConvTranspose2d: 2-3              [1554, 128, 16, 16]       409,728
│    └─BatchNorm2d: 2-4                  [1554, 128, 16, 16]       256
│    └─MaxPool2d: 2-5                    [1554, 128, 8, 8]         --
│    └─Mish: 2-6                         [1554, 128, 8, 8]         --
│    └─ConvTranspose2d: 2-7              [1554, 128, 12, 12]       409,728
│    └─BatchNorm2d: 2-8                  [1554, 128, 12, 12]       256
│    └─Mish: 2-9                         [1554, 128, 12, 12]       --
│    └─ConvTranspose2d: 2-10             [1554, 128, 24, 24]       262,272
│    └─BatchNorm2d: 2-11                 [1554, 128, 24, 24]       256
│    └─MaxPool2d: 2-12                   [1554, 128, 12, 12]       --
│    └─Mish: 2-13                        [1554, 128, 12, 12]       --
│    └─Conv2d: 2-14                      [1554, 128, 6, 6]         262,272
│    └─BatchNorm2d: 2-15                 [1554, 128, 6, 6]         256
│    └─Mish: 2-16                        [1554, 128, 6, 6]         --
│    └─Conv2d: 2-17                      [1554, 64, 3, 3]          131,136
│    └─Mish: 2-18                        [1554, 64, 3, 3]          --
├─Flatten: 1-2                           [1554, 576]               --
├─Linear: 1-3                            [1554, 288]               166,176
├─Mish: 1-4                              [1554, 288]               --
├─Dropout: 1-5                           [1554, 288]               --
├─Linear: 1-6                            [1554, 144]               41,616
├─Mish: 1-7                              [1554, 144]               --
├─Dropout: 1-8                           [1554, 144]               --
├─Linear: 1-9                            [1554, 144]               20,880
├─Mish: 1-10                             [1554, 144]               --
├─Dropout: 1-11                          [1554, 144]               --
├─Linear: 1-12                           [1554, 288]               41,760
├─Mish: 1-13                             [1554, 288]               --
├─Dropout: 1-14                          [1554, 288]               --
├─Linear: 1-15                           [1554, 9]                 2,601
==========================================================================================
Total params: 2,273,609
Trainable params: 2,273,609
Non-trainable params: 0
Total mult-adds (G): 623.73
==========================================================================================
Input size (MB): 57.29
Forward/backward pass size (MB): 3467.94
Params size (MB): 9.09
Estimated Total Size (MB): 3534.32
==========================================================================================</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train epochs: 100%|██████████| 1000/1000 [07:02&lt;00:00,  2.36epoch/s, epoch_loss=0.0121, train_acc=1, train_correct=1554.0, train_total=1554.0, val_acc=0.955, val_loss=0.124]   </code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>plot_train_curves(loss_dict_transfer)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>plot_val_curves(loss_dict_transfer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab3_files/figure-html/cell-32-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="part-d---2-pt-1" class="level3">
<h3 class="anchored" data-anchor-id="part-d---2-pt-1">Part (d) - 2 pt</h3>
<p>Report the test accuracy of your best model. How does the test accuracy compare to Part 3(d) without transfer learning?</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>transfer_model_state_path <span class="op">=</span> <span class="st">"./models/transfer/valacc_0.9504504799842834-convdial_2-lr_1e-05-momentum_0.95-batch_size_None-epoch_num_519.mdlckpt"</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>transfer_weight_loaded_model <span class="op">=</span> TransferCNN()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>transfer_weight_loaded_model.load_state_dict(torch.load(transfer_model_state_path))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>test_embeds <span class="op">=</span> torch.tensor(test_embeds.clone().detach())</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>test_accuracy, _, _, test_loss <span class="op">=</span> compute_transfer_test_performance(transfer_weight_loaded_model, (test_embeds, test_targets))</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_accuracy, test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\ericz\AppData\Local\Temp\ipykernel_9564\1952339223.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  test_embeds = torch.tensor(test_embeds.clone().detach())</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9774266365688488 0.24418403208255768</code></pre>
</div>
</div>
<p>The transfer learning model performed slightly better than the model from part 3(d), i.e., the model from 3(d) achieved <span class="math inline">\(\approx 95\%\)</span> (94.808%) accuracy on the test set while the transfer learning model achieved <span class="math inline">\(\approx 98\%\)</span> (97.516%) accuracy on the test set. So both models did exceptionally well!</p>
</section>
<section id="additional-testing-5-pt" class="level3">
<h3 class="anchored" data-anchor-id="additional-testing-5-pt">5. Additional Testing [5 pt]</h3>
<p>As a final step in testing we will be revisiting the sample images that you had collected and submitted at the start of this lab. These sample images should be untouched and will be used to demonstrate how well your model works at identifying your hand guestures.</p>
<p>Using the best transfer learning model developed in Part 4. Report the test accuracy on your sample images and how it compares to the test accuracy obtained in Part 4(d)? How well did your model do for the different hand guestures? Provide an explanation for why you think your model performed the way it did?</p>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-execution_count="62">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn off shuffling so we get the same ordering of the test set. </span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>_, _, small_test_loader <span class="op">=</span> create_dataloaders(</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    small_split_sets, use_cuda<span class="op">=</span><span class="va">True</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>small_test_embeds, small_test_targets <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span> </span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># will eval once since test set only has one batch</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(small_test_loader):</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span>, targets <span class="op">=</span> batch</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.to(<span class="st">"cuda:0"</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    small_test_embeds <span class="op">=</span> alexnet.features(<span class="bu">input</span>)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    small_test_targets <span class="op">=</span> targets</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>no_transfer_test_acc, _, _, _ <span class="op">=</span> compute_test_performance(weight_loaded_model, small_test_loader)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>transfer_test_acc, _, _, _ <span class="op">=</span> compute_transfer_test_performance(transfer_weight_loaded_model, (small_test_embeds, small_test_targets))</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(no_transfer_test_acc, transfer_test_acc)</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>preds_no_transfer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>preds_transfer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a><span class="co"># will eval once since test set only has one batch</span></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(small_test_loader):</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>    inputs, small_test_targets <span class="op">=</span> batch</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>    weight_loaded_model.<span class="bu">eval</span>()</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>    transfer_weight_loaded_model.<span class="bu">eval</span>()</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>    _, preds_no_transfer <span class="op">=</span> torch.<span class="bu">max</span>(weight_loaded_model(inputs), <span class="dv">1</span>) </span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>    _, preds_transfer <span class="op">=</span> torch.<span class="bu">max</span>(transfer_weight_loaded_model(small_test_embeds.cpu()), <span class="dv">1</span>)</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>accuracy_per_category <span class="op">=</span> {<span class="st">"no_transfer"</span>: [<span class="dv">0</span>]<span class="op">*</span><span class="dv">9</span>, <span class="st">"transfer"</span>: [<span class="dv">0</span>]<span class="op">*</span><span class="dv">9</span>}</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>letters <span class="op">=</span> [<span class="bu">chr</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">65</span>, <span class="dv">74</span>)]</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(torch.unique(small_test_targets))): </span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>    target_category_indexes <span class="op">=</span> (i <span class="op">==</span> small_test_targets) <span class="co"># where the class labels are for target i </span></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>    target_class_num <span class="op">=</span> torch.<span class="bu">sum</span>(target_category_indexes)</span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>    no_transfer_category_indexes <span class="op">=</span> (i <span class="op">==</span> preds_no_transfer)</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>    no_transfer_category_correct <span class="op">=</span> torch.<span class="bu">sum</span>(torch.logical_and(target_category_indexes, no_transfer_category_indexes))</span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>    transfer_category_indexes <span class="op">=</span> (i <span class="op">==</span> preds_transfer)</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>    transfer_category_correct <span class="op">=</span> torch.<span class="bu">sum</span>(torch.logical_and(target_category_indexes, transfer_category_indexes))</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>    accuracy_per_category[<span class="st">"no_transfer"</span>][i] <span class="op">=</span> no_transfer_category_correct<span class="op">/</span>target_class_num</span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>    accuracy_per_category[<span class="st">"transfer"</span>][i] <span class="op">=</span> transfer_category_correct<span class="op">/</span>target_class_num</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Category </span><span class="sc">{</span>letters[i]<span class="sc">}</span><span class="ss">, no transfer acc: </span><span class="sc">{</span>accuracy_per_category[<span class="st">'no_transfer'</span>][i]<span class="sc">}</span><span class="ss">, transfer acc: </span><span class="sc">{</span>accuracy_per_category[<span class="st">'transfer'</span>][i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9483870967741935 0.9741935483870968
Category A, no transfer acc: 0.9047619104385376, transfer acc: 0.9523809552192688
Category B, no transfer acc: 0.8947368264198303, transfer acc: 1.0
Category C, no transfer acc: 0.9523809552192688, transfer acc: 0.9523809552192688
Category D, no transfer acc: 1.0, transfer acc: 1.0
Category E, no transfer acc: 0.9473684430122375, transfer acc: 1.0
Category F, no transfer acc: 1.0, transfer acc: 1.0
Category G, no transfer acc: 1.0, transfer acc: 1.0
Category H, no transfer acc: 1.0, transfer acc: 1.0
Category I, no transfer acc: 0.8461538553237915, transfer acc: 0.8461538553237915</code></pre>
</div>
</div>
<p>Overall, the transfer learning model has roughly <span class="math inline">\(\approx 3\%\)</span> better performance on this test set. It performs marginally better on letters A and E, while it performs quite a bit better than on the letter B. Otherwise, both models perform very comparably. This is possibly due to the feature representations (embeddings) computed from AlexNet since it has been trained on many more images and could maybe have learned something that would help it recognize the letter B better, especially given that “B” only has 247 images in the whole dataset.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>