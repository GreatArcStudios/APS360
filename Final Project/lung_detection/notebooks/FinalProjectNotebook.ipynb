{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrTeUlx3LEwD"
      },
      "source": [
        "# Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrKGio1aoSLQ",
        "outputId": "ea4cf816-980a-4fc9-9e76-3f71218f3f6b"
      },
      "outputs": [],
      "source": [
        "! pip install pytorch_lightning \n",
        "! pip install wandb\n",
        "! pip install torchvision\n",
        "! pip install torchinfo\n",
        "! pip install self-attention-cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw1fx5--LHj8"
      },
      "source": [
        "# Imports and Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8QGGQO_ZpeKB"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import pytorch_lightning as pl\n",
        "import torch.cuda\n",
        "import wandb\n",
        "import torchvision\n",
        "import math\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torchinfo import summary\n",
        "from self_attention_cv import AxialAttentionBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi5Dnc-tLND5"
      },
      "source": [
        "## Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ZjCpErUmsLYp"
      },
      "outputs": [],
      "source": [
        "class Conv2dBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, kernel_size,\n",
        "            dilation=1, dropout=0.0, pool_size=1,\n",
        "            activations = nn.Mish\n",
        "    ):\n",
        "        super().__init__()\n",
        "        padding = dilation * (kernel_size - 1) // 2  # padding needed to maintain size\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, \n",
        "                      padding=padding, dilation=dilation),\n",
        "            nn.BatchNorm2d(out_channels, momentum=0.0735)\n",
        "        ]\n",
        "        if pool_size > 1:\n",
        "            layers.append(nn.MaxPool2d(kernel_size=pool_size))\n",
        "        if dropout > 0.0:\n",
        "            layers.append(nn.Dropout(p=dropout))\n",
        "        layers.append(activations())\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TransposeConv2dBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, kernel_size, dropout=0.0,\n",
        "            activations = nn.Mish\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size),\n",
        "            nn.BatchNorm2d(out_channels, momentum=0.0735)\n",
        "        ]\n",
        "        if dropout > 0.0:\n",
        "            layers.append(nn.Dropout(p=dropout))\n",
        "        layers.append(activations())\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class DilatedResConv2dBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, mid_channels, out_channels, kernel_size,\n",
        "            dilation=1, dropout=0.0, activations = nn.Mish\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            Conv2dBlock(in_channels, mid_channels, kernel_size, dilation=dilation),\n",
        "            Conv2dBlock(mid_channels, out_channels, kernel_size, dropout=dropout)\n",
        "        )\n",
        "\n",
        "        self.activation = activations()\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks_output = self.activation(self.blocks(x))\n",
        "        print(blocks_output.shape, x.shape)\n",
        "        x = x + blocks_output   # residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "G42MUqHDo2l9"
      },
      "outputs": [],
      "source": [
        "class ClassifierHead(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layer, n_head, feature_map_dim, input_channels=96, mid_channels = 24, output_channels=12, \n",
        "                 kernel_size=6, max_pool_kernel_size = 2, conv_layers=6, hidden_size=1024):\n",
        "        super().__init__()\n",
        "\n",
        "        axial_attn_block = AxialAttentionBlock(\n",
        "            input_channels, feature_map_dim, n_head)\n",
        "\n",
        "        # build up the modules\n",
        "        modules = [axial_attn_block for _ in range(n_layer)]\n",
        "        for i in range(conv_layers):\n",
        "            modules += [Conv2dBlock(input_channels,\n",
        "                                    mid_channels, kernel_size=kernel_size)]\n",
        "            modules += [nn.BatchNorm2d(mid_channels, momentum=0.0735)]\n",
        "            modules += [Conv2dBlock(mid_channels,\n",
        "                                    input_channels, kernel_size=kernel_size)]\n",
        "            modules += [nn.BatchNorm2d(input_channels, momentum=0.0735)]\n",
        "            # add maxpooling after every other conv layer\n",
        "            if i % 2 == 0: \n",
        "                modules += [nn.MaxPool2d(kernel_size=max_pool_kernel_size)]\n",
        "        modules += [Conv2dBlock(input_channels, output_channels, kernel_size=1)]\n",
        "        # unpack modules\n",
        "        self.trunk = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.trunk(x)\n",
        "\n",
        "\n",
        "class ClassifierTrunk(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels=1024, mid_channels=64, out_channels=96, kernel_size=3, transpose_kernel=12, dropout=0.04):\n",
        "        super().__init__()\n",
        "\n",
        "        # build up the modules\n",
        "        modules = [Conv2dBlock(\n",
        "            input_channels, out_channels, kernel_size=kernel_size)]\n",
        "        dilation = 1.0\n",
        "        for _ in range(6):\n",
        "            layer_dilation = round(dilation)\n",
        "            modules += [TransposeConv2dBlock(out_channels,\n",
        "                                             out_channels, transpose_kernel, dropout)]\n",
        "            modules += [DilatedResConv2dBlock(\n",
        "                out_channels, mid_channels, out_channels, kernel_size, layer_dilation, dropout)]\n",
        "            dilation *= 1.75\n",
        "\n",
        "        # unpack modules\n",
        "        self.head = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, input_embeds):\n",
        "        return self.head(input_embeds)\n",
        "\n",
        "\n",
        "class XRayPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layer, n_head, n_inner, dropout):\n",
        "        super().__init__()\n",
        "        self.trunk = ClassifierTrunk()\n",
        "        self.head = ClassifierHead(n_layer, n_head, n_inner, dropout)\n",
        "        # TODO: figure out the output dimensions of the trunk\n",
        "        # make same dims as head output for residual.\n",
        "        self.trunk_residual = nn.Linear()\n",
        "        self.residual_act = nn.Mish()\n",
        "        self.fc_out = nn.Linear(48, 15)\n",
        "\n",
        "    def forward(self, input_embeds):\n",
        "        z = self.trunk(input_embeds)\n",
        "        y = self.head(z)\n",
        "        y = self.residual_act(y + self.trunk_residual(z))\n",
        "        return self.fc_out(y)\n",
        "\n",
        "\n",
        "class LitContactPredictor(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_layer, n_head, n_inner, dropout,\n",
        "            augment_rc, augment_shift, lr,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=[\"model\"])\n",
        "\n",
        "        self.model = XRayPredictor(n_layer, n_head, n_inner, dropout)\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, input_seqs):\n",
        "        return self.model(input_seqs, flatten=True)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        batch = self._stochastic_augment(batch)\n",
        "        loss, batch_size = self._process_batch(batch)\n",
        "        self.log('train_loss', loss, batch_size=batch_size)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, batch_size = self._process_batch(batch)\n",
        "        self.log('val_loss', loss, batch_size=batch_size)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, batch_size = self._process_batch(batch, test=True)\n",
        "        self.log('test_loss', loss, batch_size=batch_size)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(\n",
        "            self.parameters(), lr=self.lr, momentum=0.975)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPMgaOscLQsE"
      },
      "source": [
        "## DataModule stuff "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1vyAheITIIK5"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "class LungDetectionDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, batch_size=2, num_workers=0, master_path=\"\"):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        data_dir = master_path\n",
        "        self.train = DatasetFolder(master_path + 'embeddingtrain2', \n",
        "                                   loader=torch.load, extensions=('.tensor'))\n",
        "        self.valid = DatasetFolder(master_path + 'embeddingval2', \n",
        "                                   loader=torch.load, extensions=('.tensor'))\n",
        "        self.test = DatasetFolder(master_path + 'embeddingval2', \n",
        "                                  loader=torch.load, extensions=('.tensor'))\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            self.train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            self.valid,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            self.test,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1Isg83qcH4eb"
      },
      "outputs": [],
      "source": [
        "def train_main(batch_size=128, num_workers = 4, max_epochs = 20, \n",
        "               master_path=\"\", n_layer = 5, n_head = 5, n_inner = 64,  dropout = 0.01):\n",
        "    # seed experiment\n",
        "    pl.seed_everything(seed=123)\n",
        "\n",
        "    # construct datamodule\n",
        "    datamodule = LungDetectionDataModule(batch_size=batch_size, \n",
        "                                         num_workers=num_workers, \n",
        "                                         master_path = \"\")\n",
        "    data_size = len(datamodule.train)\n",
        "\n",
        "    # construct model\n",
        "    lit_model = LitContactPredictor(seed=123, batch_size=batch_size, \n",
        "                                    num_workers=num_workers, \n",
        "                                    data_size=data_size,\n",
        "                                    n_layer= n_layer,\n",
        "                                    n_head = n_head,\n",
        "                                    n_inner = n_inner,\n",
        "                                    dropout=dropout\n",
        "                                    )\n",
        "\n",
        "    # logging\n",
        "    save_dir = pathlib.Path(__file__).parents[2]\n",
        "    logger = WandbLogger(project=\"train_xray\", log_model=\"all\", save_dir=str(save_dir))\n",
        "    logger.experiment.config[\"train_set_len\"] = len(datamodule.train)\n",
        "    logger.experiment.config[\"val_set_len\"] = len(datamodule.valid)\n",
        "    logger.experiment.config[\"batch_size\"] = batch_size\n",
        "\n",
        "    # callbacks\n",
        "    early_stopping = pl.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=40)\n",
        "    checkpointing = pl.callbacks.ModelCheckpoint(monitor=\"val_accuracy\", mode=\"min\", save_top_k=20)\n",
        "    stochastic_weighting = pl.callbacks.StochasticWeightAveraging(swa_epoch_start=0.75, \n",
        "                                                                  annealing_epochs=5, \n",
        "                                                                  swa_lrs=4.5e-4)\n",
        "    lr_monitor = pl.callbacks.LearningRateMonitor(\"step\", True)\n",
        "\n",
        "    # training\n",
        "    trainer = pl.Trainer(\n",
        "        callbacks=[early_stopping, checkpointing, stochastic_weighting, lr_monitor],\n",
        "        deterministic=True,\n",
        "        gpus=-1,\n",
        "        gradient_clip_val=15,\n",
        "        logger=logger,\n",
        "        log_every_n_steps=1,\n",
        "        enable_progress_bar=True,\n",
        "        max_epochs=max_epochs,\n",
        "    )\n",
        "\n",
        "    trainer.fit(lit_model, datamodule=datamodule)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return lit_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qn1CeGPK0S4",
        "outputId": "96146acd-a2c2-4ee4-880d-cc9bcf3bebe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXJZRk3ZLZWy"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mBSj1M68K_QY"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "#location on Google Drive\n",
        "master_path = '../data_processing/embeddings/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ct1PAvAcZjg5"
      },
      "outputs": [],
      "source": [
        "train_configs = {\"n_layer\": 2, \n",
        "                 \"n_head\": 4,\n",
        "                 \"n_inner\": 64,\n",
        "                 \"dropout\": 0.01}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykJRRos9VuxS"
      },
      "source": [
        "## Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "afmrCQnkVzmD"
      },
      "outputs": [],
      "source": [
        "test_datamodule = LungDetectionDataModule(master_path=master_path, batch_size = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "za6x6YBtWCJI"
      },
      "outputs": [],
      "source": [
        "test_train_loader = test_datamodule.train_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8owzwvPrWR3y",
        "outputId": "6e9a41bf-41ab-42d8-ef53-3c410c01b1b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 1024, 7, 7]), torch.Size([128]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_train_sample = next(iter(test_train_loader))\n",
        "test_input, test_label = test_train_sample\n",
        "test_input, test_label = test_input.cuda(), test_label.cuda()\n",
        "test_input.shape, test_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qXBrYVQWoXA",
        "outputId": "c4b5ed5b-b550-49cf-ef34-80d584e0e60e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([10,  5,  8, 11, 10, 14,  0, 13, 10, 10,  0, 14,  9, 10, 14, 12, 13, 14,\n",
              "        13,  9,  3, 11,  8,  9,  2,  0,  4,  0,  8,  4,  8, 10, 14,  9, 10,  8,\n",
              "         8, 10,  4, 10,  2, 11, 13,  2,  4,  8, 10,  8,  4,  8,  4, 11,  8,  0,\n",
              "        13,  8, 12, 10, 13, 12, 10,  8,  0,  8,  4, 10,  5, 10,  8, 10, 14, 10,\n",
              "        10,  8, 10, 13, 10,  6, 10, 13, 14, 14,  1, 10,  1, 14, 10, 14, 14,  1,\n",
              "        13,  8,  4, 11,  0,  4,  8,  0, 10, 10, 14,  8,  8,  1, 11,  0,  1,  8,\n",
              "         0,  9,  8, 11,  8, 10,  4,  4, 10, 10,  8, 14, 11,  8, 14,  8, 10, 13,\n",
              "         8, 12], device='cuda:0')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_label is a bit werid\n",
        "test_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "jcFrLXJRZuLh"
      },
      "outputs": [],
      "source": [
        "n_layer, n_head, dropout = train_configs[\"n_layer\"], train_configs[\"n_head\"], train_configs[\"dropout\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "HZti94p3ZbKK"
      },
      "outputs": [],
      "source": [
        "input_channels, mid_channels, out_channels, kernel_size, transpose_kernel, dropout_trunk = 1024, 64, 48, 5, 5, 0.04\n",
        "dryrun_trunk = ClassifierTrunk(input_channels, mid_channels, out_channels, kernel_size, transpose_kernel, dropout_trunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_MIC0jzb76d",
        "outputId": "7b496713-e400-4a92-dda9-e6aaef03f976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 48, 11, 11]) torch.Size([128, 48, 11, 11])\n",
            "torch.Size([128, 48, 15, 15]) torch.Size([128, 48, 15, 15])\n",
            "torch.Size([128, 48, 19, 19]) torch.Size([128, 48, 19, 19])\n",
            "torch.Size([128, 48, 23, 23]) torch.Size([128, 48, 23, 23])\n",
            "torch.Size([128, 48, 27, 27]) torch.Size([128, 48, 27, 27])\n",
            "torch.Size([128, 48, 31, 31]) torch.Size([128, 48, 31, 31])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "ClassifierTrunk                                    [128, 48, 31, 31]         --\n",
              "├─Sequential: 1-1                                  [128, 48, 31, 31]         --\n",
              "│    └─Conv2dBlock: 2-1                            [128, 48, 7, 7]           --\n",
              "│    │    └─Sequential: 3-1                        [128, 48, 7, 7]           1,228,944\n",
              "│    └─TransposeConv2dBlock: 2-2                   [128, 48, 11, 11]         --\n",
              "│    │    └─Sequential: 3-2                        [128, 48, 11, 11]         57,744\n",
              "│    └─DilatedResConv2dBlock: 2-3                  [128, 48, 11, 11]         --\n",
              "│    │    └─Sequential: 3-3                        [128, 48, 11, 11]         153,936\n",
              "│    │    └─Mish: 3-4                              [128, 48, 11, 11]         --\n",
              "│    └─TransposeConv2dBlock: 2-4                   [128, 48, 15, 15]         --\n",
              "│    │    └─Sequential: 3-5                        [128, 48, 15, 15]         57,744\n",
              "│    └─DilatedResConv2dBlock: 2-5                  [128, 48, 15, 15]         --\n",
              "│    │    └─Sequential: 3-6                        [128, 48, 15, 15]         153,936\n",
              "│    │    └─Mish: 3-7                              [128, 48, 15, 15]         --\n",
              "│    └─TransposeConv2dBlock: 2-6                   [128, 48, 19, 19]         --\n",
              "│    │    └─Sequential: 3-8                        [128, 48, 19, 19]         57,744\n",
              "│    └─DilatedResConv2dBlock: 2-7                  [128, 48, 19, 19]         --\n",
              "│    │    └─Sequential: 3-9                        [128, 48, 19, 19]         153,936\n",
              "│    │    └─Mish: 3-10                             [128, 48, 19, 19]         --\n",
              "│    └─TransposeConv2dBlock: 2-8                   [128, 48, 23, 23]         --\n",
              "│    │    └─Sequential: 3-11                       [128, 48, 23, 23]         57,744\n",
              "│    └─DilatedResConv2dBlock: 2-9                  [128, 48, 23, 23]         --\n",
              "│    │    └─Sequential: 3-12                       [128, 48, 23, 23]         153,936\n",
              "│    │    └─Mish: 3-13                             [128, 48, 23, 23]         --\n",
              "│    └─TransposeConv2dBlock: 2-10                  [128, 48, 27, 27]         --\n",
              "│    │    └─Sequential: 3-14                       [128, 48, 27, 27]         57,744\n",
              "│    └─DilatedResConv2dBlock: 2-11                 [128, 48, 27, 27]         --\n",
              "│    │    └─Sequential: 3-15                       [128, 48, 27, 27]         153,936\n",
              "│    │    └─Mish: 3-16                             [128, 48, 27, 27]         --\n",
              "│    └─TransposeConv2dBlock: 2-12                  [128, 48, 31, 31]         --\n",
              "│    │    └─Sequential: 3-17                       [128, 48, 31, 31]         57,744\n",
              "│    └─DilatedResConv2dBlock: 2-13                 [128, 48, 31, 31]         --\n",
              "│    │    └─Sequential: 3-18                       [128, 48, 31, 31]         153,936\n",
              "│    │    └─Mish: 3-19                             [128, 48, 31, 31]         --\n",
              "====================================================================================================\n",
              "Total params: 2,499,024\n",
              "Trainable params: 2,499,024\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 86.87\n",
              "====================================================================================================\n",
              "Input size (MB): 25.69\n",
              "Forward/backward pass size (MB): 963.61\n",
              "Params size (MB): 10.00\n",
              "Estimated Total Size (MB): 999.29\n",
              "===================================================================================================="
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(dryrun_trunk, input_data = test_input, device = \"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "aKbTR8xEz12b",
        "outputId": "25a64850-7eee-43f2-a6e2-4bc22f371314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 48, 11, 11]) torch.Size([128, 48, 11, 11])\n",
            "torch.Size([128, 48, 15, 15]) torch.Size([128, 48, 15, 15])\n",
            "torch.Size([128, 48, 19, 19]) torch.Size([128, 48, 19, 19])\n",
            "torch.Size([128, 48, 23, 23]) torch.Size([128, 48, 23, 23])\n",
            "torch.Size([128, 48, 27, 27]) torch.Size([128, 48, 27, 27])\n",
            "torch.Size([128, 48, 31, 31]) torch.Size([128, 48, 31, 31])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 48, 31, 31])"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dryrun_trunk_output = dryrun_trunk(test_input)\n",
        "dryrun_trunk_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "S2Z79a_gy1Ou"
      },
      "outputs": [],
      "source": [
        "dryrun_head = ClassifierHead(3, 8, dryrun_trunk_output.size(2), \n",
        "                             input_channels = out_channels, \n",
        "                             mid_channels= 24,  \n",
        "                             output_channels = 12, \n",
        "                             kernel_size = 4, \n",
        "                             max_pool_kernel_size = 2, \n",
        "                             conv_layers = 2, \n",
        "                             hidden_size = 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "0zN1M6Xqz0RL",
        "outputId": "1584e3b9-0f80-42f9-defc-d05d236a7f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "ClassifierHead                                     [128, 12, 12, 12]         --\n",
            "├─Sequential: 1-1                                  [128, 12, 12, 12]         --\n",
            "│    └─AxialAttentionBlock: 2-1                    [128, 48, 31, 31]         --\n",
            "│    │    └─Sequential: 3-1                        [128, 128, 31, 31]        6,400\n",
            "│    │    └─ReLU: 3-2                              [128, 128, 31, 31]        --\n",
            "│    │    └─AxialAttention: 3-3                    [3968, 128, 31]           35,792\n",
            "│    │    └─AxialAttention: 3-4                    [3968, 128, 31]           35,792\n",
            "│    │    └─ReLU: 3-5                              [3968, 128, 31]           --\n",
            "│    │    └─Sequential: 3-6                        [128, 48, 31, 31]         6,240\n",
            "│    │    └─ReLU: 3-7                              [128, 48, 31, 31]         --\n",
            "│    └─AxialAttentionBlock: 2-2                    [128, 48, 31, 31]         (recursive)\n",
            "│    │    └─Sequential: 3-8                        [128, 128, 31, 31]        (recursive)\n",
            "│    │    └─ReLU: 3-9                              [128, 128, 31, 31]        --\n",
            "│    │    └─AxialAttention: 3-10                   [3968, 128, 31]           (recursive)\n",
            "│    │    └─AxialAttention: 3-11                   [3968, 128, 31]           (recursive)\n",
            "│    │    └─ReLU: 3-12                             [3968, 128, 31]           --\n",
            "│    │    └─Sequential: 3-13                       [128, 48, 31, 31]         (recursive)\n",
            "│    │    └─ReLU: 3-14                             [128, 48, 31, 31]         --\n",
            "│    └─AxialAttentionBlock: 2-3                    [128, 48, 31, 31]         (recursive)\n",
            "│    │    └─Sequential: 3-15                       [128, 128, 31, 31]        (recursive)\n",
            "│    │    └─ReLU: 3-16                             [128, 128, 31, 31]        --\n",
            "│    │    └─AxialAttention: 3-17                   [3968, 128, 31]           (recursive)\n",
            "│    │    └─AxialAttention: 3-18                   [3968, 128, 31]           (recursive)\n",
            "│    │    └─ReLU: 3-19                             [3968, 128, 31]           --\n",
            "│    │    └─Sequential: 3-20                       [128, 48, 31, 31]         (recursive)\n",
            "│    │    └─ReLU: 3-21                             [128, 48, 31, 31]         --\n",
            "│    └─Conv2dBlock: 2-4                            [128, 24, 30, 30]         --\n",
            "│    │    └─Sequential: 3-22                       [128, 24, 30, 30]         18,504\n",
            "│    └─BatchNorm2d: 2-5                            [128, 24, 30, 30]         48\n",
            "│    └─Conv2dBlock: 2-6                            [128, 48, 29, 29]         --\n",
            "│    │    └─Sequential: 3-23                       [128, 48, 29, 29]         18,576\n",
            "│    └─BatchNorm2d: 2-7                            [128, 48, 29, 29]         96\n",
            "│    └─MaxPool2d: 2-8                              [128, 48, 14, 14]         --\n",
            "│    └─Conv2dBlock: 2-9                            [128, 24, 13, 13]         --\n",
            "│    │    └─Sequential: 3-24                       [128, 24, 13, 13]         18,504\n",
            "│    └─BatchNorm2d: 2-10                           [128, 24, 13, 13]         48\n",
            "│    └─Conv2dBlock: 2-11                           [128, 48, 12, 12]         --\n",
            "│    │    └─Sequential: 3-25                       [128, 48, 12, 12]         18,576\n",
            "│    └─BatchNorm2d: 2-12                           [128, 48, 12, 12]         96\n",
            "│    └─Conv2dBlock: 2-13                           [128, 12, 12, 12]         --\n",
            "│    │    └─Sequential: 3-26                       [128, 12, 12, 12]         612\n",
            "====================================================================================================\n",
            "Total params: 159,284\n",
            "Trainable params: 159,284\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 33.61\n",
            "====================================================================================================\n",
            "Input size (MB): 23.62\n",
            "Forward/backward pass size (MB): 10194.57\n",
            "Params size (MB): 0.64\n",
            "Estimated Total Size (MB): 10218.82\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "ClassifierHead                                     [128, 12, 12, 12]         --\n",
              "├─Sequential: 1-1                                  [128, 12, 12, 12]         --\n",
              "│    └─AxialAttentionBlock: 2-1                    [128, 48, 31, 31]         --\n",
              "│    │    └─Sequential: 3-1                        [128, 128, 31, 31]        6,400\n",
              "│    │    └─ReLU: 3-2                              [128, 128, 31, 31]        --\n",
              "│    │    └─AxialAttention: 3-3                    [3968, 128, 31]           35,792\n",
              "│    │    └─AxialAttention: 3-4                    [3968, 128, 31]           35,792\n",
              "│    │    └─ReLU: 3-5                              [3968, 128, 31]           --\n",
              "│    │    └─Sequential: 3-6                        [128, 48, 31, 31]         6,240\n",
              "│    │    └─ReLU: 3-7                              [128, 48, 31, 31]         --\n",
              "│    └─AxialAttentionBlock: 2-2                    [128, 48, 31, 31]         (recursive)\n",
              "│    │    └─Sequential: 3-8                        [128, 128, 31, 31]        (recursive)\n",
              "│    │    └─ReLU: 3-9                              [128, 128, 31, 31]        --\n",
              "│    │    └─AxialAttention: 3-10                   [3968, 128, 31]           (recursive)\n",
              "│    │    └─AxialAttention: 3-11                   [3968, 128, 31]           (recursive)\n",
              "│    │    └─ReLU: 3-12                             [3968, 128, 31]           --\n",
              "│    │    └─Sequential: 3-13                       [128, 48, 31, 31]         (recursive)\n",
              "│    │    └─ReLU: 3-14                             [128, 48, 31, 31]         --\n",
              "│    └─AxialAttentionBlock: 2-3                    [128, 48, 31, 31]         (recursive)\n",
              "│    │    └─Sequential: 3-15                       [128, 128, 31, 31]        (recursive)\n",
              "│    │    └─ReLU: 3-16                             [128, 128, 31, 31]        --\n",
              "│    │    └─AxialAttention: 3-17                   [3968, 128, 31]           (recursive)\n",
              "│    │    └─AxialAttention: 3-18                   [3968, 128, 31]           (recursive)\n",
              "│    │    └─ReLU: 3-19                             [3968, 128, 31]           --\n",
              "│    │    └─Sequential: 3-20                       [128, 48, 31, 31]         (recursive)\n",
              "│    │    └─ReLU: 3-21                             [128, 48, 31, 31]         --\n",
              "│    └─Conv2dBlock: 2-4                            [128, 24, 30, 30]         --\n",
              "│    │    └─Sequential: 3-22                       [128, 24, 30, 30]         18,504\n",
              "│    └─BatchNorm2d: 2-5                            [128, 24, 30, 30]         48\n",
              "│    └─Conv2dBlock: 2-6                            [128, 48, 29, 29]         --\n",
              "│    │    └─Sequential: 3-23                       [128, 48, 29, 29]         18,576\n",
              "│    └─BatchNorm2d: 2-7                            [128, 48, 29, 29]         96\n",
              "│    └─MaxPool2d: 2-8                              [128, 48, 14, 14]         --\n",
              "│    └─Conv2dBlock: 2-9                            [128, 24, 13, 13]         --\n",
              "│    │    └─Sequential: 3-24                       [128, 24, 13, 13]         18,504\n",
              "│    └─BatchNorm2d: 2-10                           [128, 24, 13, 13]         48\n",
              "│    └─Conv2dBlock: 2-11                           [128, 48, 12, 12]         --\n",
              "│    │    └─Sequential: 3-25                       [128, 48, 12, 12]         18,576\n",
              "│    └─BatchNorm2d: 2-12                           [128, 48, 12, 12]         96\n",
              "│    └─Conv2dBlock: 2-13                           [128, 12, 12, 12]         --\n",
              "│    │    └─Sequential: 3-26                       [128, 12, 12, 12]         612\n",
              "====================================================================================================\n",
              "Total params: 159,284\n",
              "Trainable params: 159,284\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 33.61\n",
              "====================================================================================================\n",
              "Input size (MB): 23.62\n",
              "Forward/backward pass size (MB): 10194.57\n",
              "Params size (MB): 0.64\n",
              "Estimated Total Size (MB): 10218.82\n",
              "===================================================================================================="
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(dryrun_head, input_data = dryrun_trunk_output, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL8bJyt3X8N9"
      },
      "outputs": [],
      "source": [
        "# model dryrun \n",
        "dryrun_model = LitContactPredictor(seed=123, batch_size=2, \n",
        "                                    num_workers=4, data_size=2, \n",
        "                                   n_layer = 5, n_head = 5, \n",
        "                                   n_inner = 64,  dropout = 0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWLrZN48Vvwx"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd103kmqI7gQ"
      },
      "outputs": [],
      "source": [
        "model = train_main(master_path = master_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Pytorch20",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16 (default, Mar  2 2023, 03:18:16) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1b359cbce28cfdeeac83dcaa24271bc1308b37fc25c47ff1f45c1f32b5e130d7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
